{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f93b7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.612 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "/home/daiyuxin/miniconda3/envs/mindspore/lib/python3.9/site-packages/Cython/Compiler/Main.py:381: FutureWarning: Cython directive 'language_level' not set, using '3str' for now (Py3). This has changed from earlier releases! File: /home/daiyuxin/miniconda3/envs/mindspore/lib/python3.9/site-packages/mindnlp/transformers/models/graphormer/algos_graphormer.pyx\n",
      "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "In file included from /home/daiyuxin/miniconda3/envs/mindspore/lib/python3.9/site-packages/numpy/core/include/numpy/ndarraytypes.h:1929:0,\n",
      "                 from /home/daiyuxin/miniconda3/envs/mindspore/lib/python3.9/site-packages/numpy/core/include/numpy/ndarrayobject.h:12,\n",
      "                 from /home/daiyuxin/miniconda3/envs/mindspore/lib/python3.9/site-packages/numpy/core/include/numpy/arrayobject.h:5,\n",
      "                 from /home/daiyuxin/.pyxbld/temp.linux-x86_64-cpython-39/home/daiyuxin/miniconda3/envs/mindspore/lib/python3.9/site-packages/mindnlp/transformers/models/graphormer/algos_graphormer.c:1210:\n",
      "/home/daiyuxin/miniconda3/envs/mindspore/lib/python3.9/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\n",
      " #warning \"Using deprecated NumPy API, disable it with \" \\\n",
      "  ^~~~~~~\n"
     ]
    }
   ],
   "source": [
    "from mindnlp.transformers import AutoModelForSeq2SeqLM\n",
    "from mindnlp.peft import get_peft_config, get_peft_model, get_peft_model_state_dict, PrefixTuningConfig, TaskType\n",
    "from mindnlp.dataset import load_dataset\n",
    "\n",
    "import mindspore\n",
    "from mindspore import ops\n",
    "import os\n",
    "\n",
    "from mindnlp.transformers import AutoTokenizer\n",
    "from mindnlp.modules.optimization import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_name_or_path = \"t5-large\"\n",
    "tokenizer_name_or_path = \"t5-large\"\n",
    "checkpoint_name = \"financial_sentiment_analysis_prefix_tuning_v1.ckpt\"\n",
    "max_length = 128\n",
    "lr = 1e-2\n",
    "num_epochs = 5\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d0850ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 983,040 || all params: 738,651,136 || trainable%: 0.13308583065659835\n"
     ]
    }
   ],
   "source": [
    "# creating model\n",
    "peft_config = PrefixTuningConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, num_virtual_tokens=20)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ee2babf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mindspore.dataset.config.set_seed(123)\n",
    "# loading dataset\n",
    "dataset = load_dataset(\"financial_phrasebank\", \"sentences_allagree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1ecd7a0-2e48-405e-940b-001a96ccc18f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative', 'neutral', 'positive']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = dataset.source.ds.features[\"label\"].names\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cbbf475-89b6-4368-a7eb-730dc532558e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(1935545:140581710448448,MainProcess):2024-05-28-16:30:53.639.132 [mindspore/dataset/engine/datasets.py:1203] Dataset is shuffled before split.\n"
     ]
    }
   ],
   "source": [
    "train_dataset, validation_dataset = dataset.shuffle(64).split([0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a89748b4-3193-4419-be39-2dd7907e349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_text_label(sentence, label):\n",
    "    return sentence, label, classes[label.item()]\n",
    "\n",
    "train_dataset = train_dataset.map(add_text_label, ['sentence', 'label'], ['sentence', 'label', 'text_label'])\n",
    "validation_dataset = validation_dataset.map(add_text_label, ['sentence', 'label'], ['sentence', 'label', 'text_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "396f46c4-45ba-4d45-a98f-fff173038b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': Tensor(shape=[], dtype=String, value= 'The gross area of the Innova 2 project will be about 10,000 sq m ( 107,600 sq ft ) .'),\n",
       " 'label': Tensor(shape=[], dtype=Int64, value= 1),\n",
       " 'text_label': Tensor(shape=[], dtype=String, value= 'neutral')}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_dataset.create_dict_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "603ea9ef-88dd-4c8e-bd10-df7a64940b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daiyuxin/miniconda3/envs/mindspore/lib/python3.9/site-packages/mindnlp/transformers/models/t5/tokenization_t5_fast.py:162: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86d29e1a-eb31-48bf-a6e3-bc3cfa26cde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mindnlp.dataset import BaseMapFunction\n",
    "from threading import Lock\n",
    "lock = Lock()\n",
    "\n",
    "class MapFunc(BaseMapFunction):\n",
    "    def __call__(self, sentence, label, text_label):\n",
    "        lock.acquire()\n",
    "        model_inputs = tokenizer(sentence, max_length=max_length, padding=\"max_length\", truncation=True)\n",
    "        labels = tokenizer(text_label, max_length=3, padding=\"max_length\", truncation=True)\n",
    "        lock.release()\n",
    "        labels = labels['input_ids']\n",
    "        labels = np.where(np.equal(labels, tokenizer.pad_token_id), -100, labels)\n",
    "        return model_inputs['input_ids'], model_inputs['attention_mask'], labels\n",
    "\n",
    "\n",
    "def get_dataset(dataset, tokenizer, shuffle=True):\n",
    "    input_colums=['sentence', 'label', 'text_label']\n",
    "    output_columns=['input_ids', 'attention_mask', 'labels']\n",
    "    dataset = dataset.map(MapFunc(input_colums, output_columns),\n",
    "                          input_colums, output_columns)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(64)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = get_dataset(train_dataset, tokenizer)\n",
    "eval_dataset = get_dataset(validation_dataset, tokenizer, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b955679-aab8-4f82-91cf-06cf7e3db1c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': Tensor(shape=[8, 128], dtype=Int64, value=\n",
       " [[  37, 8690,  616 ...    0,    0,    0],\n",
       "  [6851,   48,  993 ...    0,    0,    0],\n",
       "  [  37, 3334,  349 ...    0,    0,    0],\n",
       "  ...\n",
       "  [4980,   15,    9 ...    0,    0,    0],\n",
       "  [9273, 2699, 3751 ...    0,    0,    0],\n",
       "  [  37, 8148,  349 ...    0,    0,    0]]),\n",
       " 'attention_mask': Tensor(shape=[8, 128], dtype=Int64, value=\n",
       " [[1, 1, 1 ... 0, 0, 0],\n",
       "  [1, 1, 1 ... 0, 0, 0],\n",
       "  [1, 1, 1 ... 0, 0, 0],\n",
       "  ...\n",
       "  [1, 1, 1 ... 0, 0, 0],\n",
       "  [1, 1, 1 ... 0, 0, 0],\n",
       "  [1, 1, 1 ... 0, 0, 0]]),\n",
       " 'labels': Tensor(shape=[8, 3], dtype=Int64, value=\n",
       " [[7163,    1, -100],\n",
       "  [7163,    1, -100],\n",
       "  [7163,    1, -100],\n",
       "  ...\n",
       "  [7163,    1, -100],\n",
       "  [1465,    1, -100],\n",
       "  [7163,    1, -100]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_dataset.create_dict_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f733a3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer and lr scheduler\n",
    "optimizer = mindspore.experimental.optim.AdamW(model.trainable_params(), lr=lr)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_dataset) * num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b3a4090",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 255/255 [03:12<00:00,  1.32it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 29/29 [00:07<00:00,  3.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0: train_ppl=Tensor(shape=[], dtype=Float32, value= 4.89876) train_epoch_loss=Tensor(shape=[], dtype=Float32, value= 1.58898) eval_ppl=Tensor(shape=[], dtype=Float32, value= 1.16772) eval_epoch_loss=Tensor(shape=[], dtype=Float32, value= 0.155056)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 255/255 [03:10<00:00,  1.34it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 29/29 [00:06<00:00,  4.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1: train_ppl=Tensor(shape=[], dtype=Float32, value= 1.11439) train_epoch_loss=Tensor(shape=[], dtype=Float32, value= 0.108311) eval_ppl=Tensor(shape=[], dtype=Float32, value= 1.09277) eval_epoch_loss=Tensor(shape=[], dtype=Float32, value= 0.0887188)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 255/255 [03:16<00:00,  1.30it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 29/29 [00:07<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=2: train_ppl=Tensor(shape=[], dtype=Float32, value= 1.08868) train_epoch_loss=Tensor(shape=[], dtype=Float32, value= 0.0849638) eval_ppl=Tensor(shape=[], dtype=Float32, value= 1.07994) eval_epoch_loss=Tensor(shape=[], dtype=Float32, value= 0.0769094)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 255/255 [03:13<00:00,  1.32it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 29/29 [00:07<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=3: train_ppl=Tensor(shape=[], dtype=Float32, value= 1.07052) train_epoch_loss=Tensor(shape=[], dtype=Float32, value= 0.0681448) eval_ppl=Tensor(shape=[], dtype=Float32, value= 1.05079) eval_epoch_loss=Tensor(shape=[], dtype=Float32, value= 0.0495383)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 255/255 [02:58<00:00,  1.43it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 29/29 [00:06<00:00,  4.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=4: train_ppl=Tensor(shape=[], dtype=Float32, value= 1.06582) train_epoch_loss=Tensor(shape=[], dtype=Float32, value= 0.0637434) eval_ppl=Tensor(shape=[], dtype=Float32, value= 1.04741) eval_epoch_loss=Tensor(shape=[], dtype=Float32, value= 0.0463233)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# training and evaluation\n",
    "def forward_fn(**batch):\n",
    "    outputs = model(**batch)\n",
    "    loss = outputs.loss\n",
    "    return loss\n",
    "\n",
    "grad_fn = mindspore.value_and_grad(forward_fn, None, model.trainable_params())\n",
    "\n",
    "def train_step(**batch):\n",
    "    loss, grads = grad_fn(**batch)\n",
    "    optimizer(grads)\n",
    "    return loss\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.set_train()\n",
    "    total_loss = 0\n",
    "    train_total_size = train_dataset.get_dataset_size()\n",
    "    for step, batch in enumerate(tqdm(train_dataset.create_dict_iterator(), total=train_total_size)):\n",
    "        loss = train_step(**batch)\n",
    "        total_loss += loss.float()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    model.set_train(False)\n",
    "    eval_loss = 0\n",
    "    eval_preds = []\n",
    "    eval_total_size = eval_dataset.get_dataset_size()\n",
    "    for step, batch in enumerate(tqdm(eval_dataset.create_dict_iterator(), total=eval_total_size)):\n",
    "        with mindspore._no_grad():\n",
    "            outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        eval_loss += loss.float()\n",
    "        eval_preds.extend(\n",
    "            tokenizer.batch_decode(ops.argmax(outputs.logits, -1).asnumpy(), skip_special_tokens=True)\n",
    "        )\n",
    "\n",
    "    eval_epoch_loss = eval_loss / len(eval_dataset)\n",
    "    eval_ppl = ops.exp(eval_epoch_loss)\n",
    "    train_epoch_loss = total_loss / len(train_dataset)\n",
    "    train_ppl = ops.exp(train_epoch_loss)\n",
    "    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cafa67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy=93.80530973451327 % on the evaluation dataset\n",
      "eval_preds[:10]=['neutral', 'neutral', 'neutral', 'neutral', 'positive positive', 'neutral', 'neutral', 'positive', 'positive', 'positive']\n",
      "ground_truth[:10]=['neutral', 'neutral', 'neutral', 'neutral', 'positive', 'neutral', 'neutral', 'positive', 'positive', 'positive']\n"
     ]
    }
   ],
   "source": [
    "# print accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "ground_truth = []\n",
    "\n",
    "for pred, data in zip(eval_preds, validation_dataset.create_dict_iterator(output_numpy=True)):\n",
    "    true = str(data['text_label'])\n",
    "    ground_truth.append(true)\n",
    "    if pred.strip() == true.strip():\n",
    "        correct += 1\n",
    "    total += 1\n",
    "accuracy = correct / total * 100\n",
    "print(f\"{accuracy=} % on the evaluation dataset\")\n",
    "print(f\"{eval_preds[:10]=}\")\n",
    "print(f\"{ground_truth[:10]=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26e88eb1-3d24-488b-ad62-0c18b81b861e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tensor(shape=[8, 128], dtype=Int64, value=\n",
       " [[   37,   804,   594 ...     0,     0,     0],\n",
       "  [   37,  3829,    29 ...     0,     0,     0],\n",
       "  [   37,   349,     3 ...     0,     0,     0],\n",
       "  ...\n",
       "  [   94,    19,  1316 ...     0,     0,     0],\n",
       "  [ 1615,    59, 10006 ...     0,     0,     0],\n",
       "  [   37,  1170,    13 ...     0,     0,     0]]),\n",
       " Tensor(shape=[8, 128], dtype=Int64, value=\n",
       " [[1, 1, 1 ... 0, 0, 0],\n",
       "  [1, 1, 1 ... 0, 0, 0],\n",
       "  [1, 1, 1 ... 0, 0, 0],\n",
       "  ...\n",
       "  [1, 1, 1 ... 0, 0, 0],\n",
       "  [1, 1, 1 ... 0, 0, 0],\n",
       "  [1, 1, 1 ... 0, 0, 0]]),\n",
       " Tensor(shape=[8, 3], dtype=Int64, value=\n",
       " [[7163,    1, -100],\n",
       "  [7163,    1, -100],\n",
       "  [7163,    1, -100],\n",
       "  ...\n",
       "  [7163,    1, -100],\n",
       "  [7163,    1, -100],\n",
       "  [1465,    1, -100]])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(eval_dataset.create_tuple_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8de6005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving model\n",
    "peft_model_id = f\"{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\"\n",
    "model.save_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd20cd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8M\tt5-large_PREFIX_TUNING_SEQ_2_SEQ_LM/adapter_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "ckpt = f\"{peft_model_id}/adapter_model.ckpt\"\n",
    "!du -h $ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76c2fc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindnlp.peft import PeftModel, PeftConfig\n",
    "\n",
    "peft_model_id = f\"{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37d712ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral\n",
      "{'input_ids': Tensor(shape=[1, 2], dtype=Int64, value=\n",
      "[[7163,    1]]), 'attention_mask': Tensor(shape=[1, 2], dtype=Int64, value=\n",
      "[[1, 1]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daiyuxin/miniconda3/envs/mindspore/lib/python3.9/site-packages/mindnlp/transformers/generation/utils.py:1402: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://hf-mirror.com/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0 7163    1]]\n",
      "['neutral']\n"
     ]
    }
   ],
   "source": [
    "model.set_train(False)\n",
    "example = next(validation_dataset.create_dict_iterator(output_numpy=True))\n",
    "\n",
    "print(example['text_label'])\n",
    "inputs = tokenizer(example['text_label'], return_tensors=\"ms\")\n",
    "print(inputs)\n",
    "\n",
    "with mindspore._no_grad():\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=10)\n",
    "    print(outputs)\n",
    "    print(tokenizer.batch_decode(outputs.asnumpy(), skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcadb29e-9a09-4ceb-ba5f-131595833d85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindspore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
