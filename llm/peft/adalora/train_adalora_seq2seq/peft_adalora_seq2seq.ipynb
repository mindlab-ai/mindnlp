{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "os.environ[\"PATH\"] = \"/home/daiyuxin/miniconda3/envs/lyk_ms2.3/bin:/usr/local/cuda-11.6/bin:/home/daiyuxin/.vscode-server/bin/8b3775030ed1a69b13e4f4c628c612102e30a681/bin/remote-cli:/home/daiyuxin/.local/bin:/home/daiyuxin/bin:/usr/local/cuda-11.6/bin:/home/daiyuxin/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\"\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = \"/usr/local/cuda-11.6/lib64\"\n",
    "os.environ[\"CUDA_HOME\"] = \"/usr/local/cuda-11.6\"\n",
    "import mindspore\n",
    "import datasets\n",
    "from tqdm import tqdm\n",
    "from mindnlp.transformers import BartTokenizer, AutoModelForSeq2SeqLM\n",
    "import numpy as np\n",
    "\n",
    "from mindnlp.peft import AdaLoraConfig, PeftConfig, PeftModel, TaskType, get_peft_model\n",
    "from mindspore.dataset import GeneratorDataset\n",
    "\n",
    "mindspore.set_context(device_target=\"GPU\")\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "model_name_or_path = \"facebook/bart-base\"\n",
    "tokenizer_name_or_path = \"facebook/bart-base\"\n",
    "\n",
    "checkpoint_name = \"financial_sentiment_analysis_lora_v1.pt\"\n",
    "text_column = \"sentence\"\n",
    "label_column = \"text_label\"\n",
    "max_length = 128\n",
    "lr = 1e-3\n",
    "num_epochs = 8\n",
    "batch_size = 8\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "# creating model\n",
    "peft_config = AdaLoraConfig(\n",
    "    init_r=12,\n",
    "    target_r=8,\n",
    "    beta1=0.85,\n",
    "    beta2=0.85,\n",
    "    tinit=200,\n",
    "    tfinal=1000,\n",
    "    deltaT=10,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    ")\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "# download dataset\n",
    "!wget https://hf-mirror.com/datasets/financial_phrasebank/resolve/main/data/FinancialPhraseBank-v1.0.zip\n",
    "!unzip FinancialPhraseBank-v1.0.zip"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "def process_dataset(source,   batch_size=32, shuffle=False):\n",
    "\n",
    "    column_names = ['input_ids', 'attention_mask','labels','text_labels']\n",
    "    \n",
    "    dataset = GeneratorDataset(source, column_names=column_names, shuffle=shuffle)\n",
    "    # transforms\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "class MSDataset:\n",
    "    def __init__(self, filepath,tokenizer,max_length):\n",
    "        self.path = filepath\n",
    "        self.sentences = []\n",
    "        self.labels = []\n",
    "        self.text_labels = []\n",
    "        self._load()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def _load(self):\n",
    "        label_mapping = {\n",
    "            \"negative\": 0,\n",
    "            \"neutral\": 1,\n",
    "            \"positive\": 2\n",
    "        }\n",
    "        with open(self.path, encoding=\"iso-8859-1\") as f:\n",
    "            for line in f:\n",
    "                sentence, label_text = line.strip().split(\"@\")\n",
    "                self.sentences.append(sentence)\n",
    "                self.labels.append(label_mapping[label_text])\n",
    "                self.text_labels.append(label_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.sentences[index]\n",
    "        text_labels = self.text_labels[index]\n",
    "        model_inputs = self.tokenizer(sentence, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"np\")\n",
    "        labels = self.tokenizer(text_labels, max_length=3, padding=\"max_length\", truncation=True, return_tensors=\"np\")\n",
    "        labels = labels[\"input_ids\"]\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return model_inputs['input_ids'], model_inputs['attention_mask'], labels,self.text_labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "tokenizer = BartTokenizer.from_pretrained(model_name_or_path)\n",
    "dataset = process_dataset(MSDataset(\"./FinancialPhraseBank-v1.0/Sentences_AllAgree.txt\",tokenizer,max_length),batch_size=batch_size)\n",
    "\n",
    "train_dataset, eval_dataset = dataset.split([0.9, 0.1])\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "optimizer = mindspore.nn.AdamWeightDecay(model.trainable_params(), learning_rate=lr)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "print(next(train_dataset.create_dict_iterator()))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "model.base_model.peft_config[\"default\"].total_step = len(train_dataset) * num_epochs\n",
    "from mindspore import Tensor\n",
    "\n",
    "num_batches = len(train_dataset)\n",
    "num_batches_eval = len(eval_dataset)\n",
    "                       \n",
    "def forward_fn(input_ids,attention_mask,labels ):\n",
    "        output = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "        return output.loss, output.logits\n",
    "    \n",
    "grad_fn = mindspore.value_and_grad(\n",
    "        forward_fn, None,model.trainable_params(), has_aux=True,return_ids=True\n",
    "    )\n",
    "global_step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.set_train(True)\n",
    "    total_loss, total_step = 0, 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with tqdm(total=num_batches) as t:\n",
    "        for step, (input_ids,attention_mask,labels,_) in enumerate(train_dataset):\n",
    "            input_ids  = input_ids.squeeze(axis=1)\n",
    "            labels  = labels.squeeze(axis=1)\n",
    "            attention_mask = attention_mask.squeeze(axis=1)\n",
    "            (loss, logits), grad = grad_fn(input_ids,attention_mask,labels)\n",
    "            gradient = [g for _, g in grad]\n",
    "            gradient = tuple(gradient)\n",
    "            optimizer(gradient)\n",
    "            model.base_model.update_and_allocate(global_step, grad)\n",
    "            total_loss += loss.asnumpy()\n",
    "            total_step += 1\n",
    "            global_step += 1\n",
    "            curr_loss = total_loss / total_step\n",
    "            t.set_postfix({'train-loss': f'{curr_loss:.2f}'})\n",
    "            t.update(1)\n",
    "    model.set_train(False)\n",
    "    eval_loss = 0\n",
    "    total_step = 0\n",
    "    eval_preds = []\n",
    "    text_labels = []\n",
    "    with tqdm(total=num_batches_eval) as t:\n",
    "        for step, (input_ids,attention_mask,labels,text) in enumerate(eval_dataset):\n",
    "            input_ids = input_ids.squeeze(axis=1)\n",
    "            labels = labels.squeeze(axis=1)\n",
    "            attention_mask = attention_mask.squeeze(axis=1)\n",
    "            outputs = model(input_ids=input_ids,attention_mask=attention_mask,labels=labels)\n",
    "            loss = outputs.loss\n",
    "            eval_loss += loss.asnumpy()\n",
    "            total_step += 1           \n",
    "            eval_loss = total_loss / total_step\n",
    "            eval_preds.extend(\n",
    "                tokenizer.batch_decode(np.argmax(outputs.logits.asnumpy(), -1), skip_special_tokens=True)\n",
    "            )\n",
    "            text_str = str(text.asnumpy())\n",
    "            text_str = text_str.replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"\").replace(\"'\", \"\")\n",
    "            labels = text_str.split(\" \")\n",
    "            text_labels.extend(labels)\n",
    "            t.set_postfix({'eval-loss': f'{eval_loss:.2f}'})\n",
    "            t.update(1)\n",
    "    for pred, text_label in zip(eval_preds, text_labels):\n",
    "        if pred.strip() == text_label.strip():\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"{accuracy=} % on the evaluation dataset\")\n",
    "    eval_epoch_loss = eval_loss / eval_dataset.get_dataset_size()\n",
    "    eval_ppl = np.exp(eval_epoch_loss)\n",
    "    train_epoch_loss = total_loss / train_dataset.get_dataset_size()\n",
    "    train_ppl = np.exp(train_epoch_loss)\n",
    "    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "peft_model_id = f\"{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\"\n",
    "model.save_pretrained(peft_model_id)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "ckpt = f\"{peft_model_id}/adapter_model.ckpt\"\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "peft_model_id = f\"{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\"\n",
    "\n",
    "new_config = PeftConfig.from_pretrained(peft_model_id)\n",
    "new_model = AutoModelForSeq2SeqLM.from_pretrained(new_config.base_model_name_or_path)\n",
    "new_model = PeftModel.from_pretrained(new_model, peft_model_id)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "eval_preds = []\n",
    "text_labels = []\n",
    "for step, (input_ids,attention_mask,labels,text) in enumerate(eval_dataset):\n",
    "            input_ids = input_ids.squeeze(axis=1)\n",
    "            labels = labels.squeeze(axis=1)\n",
    "            attention_mask = attention_mask.squeeze(axis=1)\n",
    "            outputs = model(input_ids=input_ids,attention_mask=attention_mask,labels=labels)\n",
    "            eval_preds.extend(\n",
    "                tokenizer.batch_decode(np.argmax(outputs.logits.asnumpy(), -1), skip_special_tokens=True)\n",
    "            )\n",
    "            text_str = str(text.asnumpy())\n",
    "            text_str = text_str.replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"\").replace(\"'\", \"\").replace(\"\\n\", \"\")\n",
    "            labels = text_str.split(\" \")\n",
    "            text_labels.extend(labels)\n",
    "for pred, text_label in zip(eval_preds, text_labels):\n",
    "    print(f\"{pred=} {text_label=}\")"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lyk_ms2.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
