{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "import mindspore\n",
    "import datasets\n",
    "from tqdm import tqdm\n",
    "from mindnlp.transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import numpy as np\n",
    "\n",
    "from mindnlp.peft import AdaLoraConfig, PeftConfig, PeftModel, TaskType, get_peft_model\n",
    "from mindspore.dataset import GeneratorDataset\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "device = \"cuda\"\n",
    "model_name_or_path = \"facebook/bart-base\"\n",
    "tokenizer_name_or_path = \"facebook/bart-base\"\n",
    "\n",
    "checkpoint_name = \"financial_sentiment_analysis_lora_v1.pt\"\n",
    "text_column = \"sentence\"\n",
    "label_column = \"text_label\"\n",
    "max_length = 128\n",
    "lr = 1e-3\n",
    "num_epochs = 8\n",
    "batch_size = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating model\n",
    "peft_config = AdaLoraConfig(\n",
    "    init_r=12,\n",
    "    target_r=8,\n",
    "    beta1=0.85,\n",
    "    beta2=0.85,\n",
    "    tinit=200,\n",
    "    tfinal=1000,\n",
    "    deltaT=10,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(source,   batch_size=32, shuffle=False):\n",
    "\n",
    "    column_names = ['input_ids', 'attention_mask','labels','text_labels']\n",
    "    \n",
    "    dataset = GeneratorDataset(source, column_names=column_names, shuffle=shuffle)\n",
    "    # transforms\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "class MSDataset:\n",
    "    def __init__(self, filepath,tokenizer,max_length):\n",
    "        self.path = filepath\n",
    "        self.sentences = []\n",
    "        self.labels = []\n",
    "        self.text_labels = []\n",
    "        self._load()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def _load(self):\n",
    "        label_mapping = {\n",
    "            \"negative\": 0,\n",
    "            \"neutral\": 1,\n",
    "            \"positive\": 2\n",
    "        }\n",
    "        with open(self.path, encoding=\"iso-8859-1\") as f:\n",
    "            for line in f:\n",
    "                sentence, label_text = line.strip().split(\"@\")\n",
    "                self.sentences.append(sentence)\n",
    "                self.labels.append(label_mapping[label_text])\n",
    "                self.text_labels.append(label_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.sentences[index]\n",
    "        text_labels = self.text_labels[index]\n",
    "        model_inputs = self.tokenizer(sentence, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"np\")\n",
    "        labels = self.tokenizer(text_labels, max_length=3, padding=\"max_length\", truncation=True, return_tensors=\"np\")\n",
    "        labels = labels[\"input_ids\"]\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return model_inputs['input_ids'], model_inputs['attention_mask'], labels,self.text_labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "dataset = process_dataset(MSDataset(\"/tmp/code/dataset/data/FinancialPhraseBank-v1.0/Sentences_AllAgree.txt\",tokenizer,max_length),batch_size=batch_size)\n",
    "\n",
    "train_dataset, eval_dataset = dataset.split([0.9, 0.1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = mindspore.nn.AdamWeightDecay(model.trainable_params(), learning_rate=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(train_dataset.create_dict_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.base_model.peft_config[\"default\"].total_step = len(train_dataloader) * num_epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import Tensor\n",
    "\n",
    "num_batches = len(train_dataset)\n",
    "num_batches_eval = len(eval_dataset)\n",
    "                       \n",
    "def forward_fn(input_ids,attention_mask,labels ):\n",
    "        output = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "        return output.loss, output.logits\n",
    "    \n",
    "grad_fn = mindspore.value_and_grad(\n",
    "        forward_fn, None,optimizer.parameters, has_aux=True\n",
    "    )\n",
    "global_step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.set_train(True)\n",
    "    total_loss, total_step = 0, 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with tqdm(total=num_batches) as t:\n",
    "        for step, (input_ids,attention_mask,labels,_) in enumerate(train_dataset):\n",
    "            input_ids  = input_ids.squeeze(axis=1)\n",
    "            labels  = labels.squeeze(axis=1)\n",
    "            attention_mask = attention_mask.squeeze(axis=1)\n",
    "            (loss, logits), grad = grad_fn(input_ids,attention_mask,labels)\n",
    "            optimizer(grad)\n",
    "            model.base_model.update_and_allocate(global_step, grad)\n",
    "            total_loss += loss.asnumpy()\n",
    "            total_step += 1\n",
    "            global_step += 1\n",
    "            curr_loss = total_loss / total_step\n",
    "            t.set_postfix({'train-loss': f'{curr_loss:.2f}'})\n",
    "            t.update(1)\n",
    "    model.set_train(False)\n",
    "    eval_loss = 0\n",
    "    total_step = 0\n",
    "    eval_preds = []\n",
    "    text_labels = []\n",
    "    with tqdm(total=num_batches_eval) as t:\n",
    "        for step, (input_ids,attention_mask,labels,text) in enumerate(eval_dataset):\n",
    "            input_ids = input_ids.squeeze(axis=1)\n",
    "            labels = labels.squeeze(axis=1)\n",
    "            attention_mask = attention_mask.squeeze(axis=1)\n",
    "            outputs = model(input_ids=input_ids,attention_mask=attention_mask,labels=labels)\n",
    "            loss = outputs.loss\n",
    "            eval_loss += loss.asnumpy()\n",
    "            total_step += 1           \n",
    "            eval_loss = total_loss / total_step\n",
    "            eval_preds.extend(\n",
    "                tokenizer.batch_decode(np.argmax(outputs.logits.asnumpy(), -1), skip_special_tokens=True)\n",
    "            )\n",
    "            text_str = str(text.asnumpy())\n",
    "            text_str = text_str.replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"\").replace(\"'\", \"\")\n",
    "            labels = text_str.split(\" \")\n",
    "            text_labels.extend(labels)\n",
    "            t.set_postfix({'eval-loss': f'{eval_loss:.2f}'})\n",
    "            t.update(1)\n",
    "    for pred, text_label in zip(eval_preds, text_labels):\n",
    "        if pred.strip() == text_label.strip():\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"{accuracy=} % on the evaluation dataset\")\n",
    "    eval_epoch_loss = eval_loss / eval_dataset.get_dataset_size()\n",
    "    eval_ppl = np.exp(eval_epoch_loss)\n",
    "    train_epoch_loss = total_loss / train_dataset.get_dataset_size()\n",
    "    train_ppl = np.exp(train_epoch_loss)\n",
    "    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_id = f\"{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\"\n",
    "model.save_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = f\"{peft_model_id}/adapter_model.bin\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_id = f\"{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 13\n",
    "def load_dataset(filepath):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    with open(filepath, encoding=\"iso-8859-1\") as f:\n",
    "        for line in f:\n",
    "            sentence, label = line.strip().split(\"@\")\n",
    "            sentences.append(sentence)\n",
    "            labels.append(label)\n",
    "    \n",
    "    dataset = datasets.Dataset.from_dict({\n",
    "        \"sentence\": sentences,\n",
    "        \"label\": labels,\n",
    "    })\n",
    "    return dataset\n",
    "\n",
    "dataset = load_dataset(\"/tmp/code/dataset/data/FinancialPhraseBank-v1.0/Sentences_AllAgree.txt\")\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "dataset[\"validation\"] = dataset[\"test\"]\n",
    "del dataset[\"test\"]\n",
    "\n",
    "inputs = tokenizer(dataset[\"validation\"][text_column][i], return_tensors=\"ms\")\n",
    "print(dataset[\"validation\"][text_column][i])\n",
    "print(inputs)\n",
    "\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=10)\n",
    "print(outputs)\n",
    "print(tokenizer.batch_decode(outputs ,skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
