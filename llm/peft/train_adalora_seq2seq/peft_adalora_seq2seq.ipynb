{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(873872:140383887885504,MainProcess):2024-04-25-20:51:53.627.27 [mindspore/run_check/_check_version.py:102] MindSpore version 2.3.0rc1 and cuda version 12.2.140 does not match, CUDA version [['10.1', '11.1', '11.6']] are supported by MindSpore officially. Please refer to the installation guide for version matching information: https://www.mindspore.cn/install.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daiyuxin/miniconda3/envs/lyk_ms2.3/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.612 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "import mindspore\n",
    "import datasets\n",
    "from tqdm import tqdm\n",
    "from mindnlp.transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import numpy as np\n",
    "\n",
    "from mindnlp.peft import AdaLoraConfig, PeftConfig, PeftModel, TaskType, get_peft_model\n",
    "from mindspore.dataset import GeneratorDataset\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "device = \"cuda\"\n",
    "model_name_or_path = \"facebook/bart-base\"\n",
    "tokenizer_name_or_path = \"facebook/bart-base\"\n",
    "\n",
    "checkpoint_name = \"financial_sentiment_analysis_lora_v1.pt\"\n",
    "text_column = \"sentence\"\n",
    "label_column = \"text_label\"\n",
    "max_length = 128\n",
    "lr = 1e-3\n",
    "num_epochs = 8\n",
    "batch_size = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,434,176 || all params: 141,854,688 || trainable%: 1.715964438200308\n"
     ]
    }
   ],
   "source": [
    "# creating model\n",
    "peft_config = AdaLoraConfig(\n",
    "    init_r=12,\n",
    "    target_r=8,\n",
    "    beta1=0.85,\n",
    "    beta2=0.85,\n",
    "    tinit=200,\n",
    "    tfinal=1000,\n",
    "    deltaT=10,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-25 20:54:44--  https://hf-mirror.com/datasets/financial_phrasebank/resolve/main/data/FinancialPhraseBank-v1.0.zip\n",
      "Resolving hf-mirror.com (hf-mirror.com)... 153.121.57.40, 160.16.199.204, 133.242.169.68\n",
      "Connecting to hf-mirror.com (hf-mirror.com)|153.121.57.40|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.hf-mirror.com/datasets/financial_phrasebank/0e1a06c4900fdae46091d031068601e3773ba067c7cecb5b0da1dcba5ce989a6?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27FinancialPhraseBank-v1.0.zip%3B+filename%3D%22FinancialPhraseBank-v1.0.zip%22%3B&response-content-type=application%2Fzip&Expires=1714308541&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNDMwODU0MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9kYXRhc2V0cy9maW5hbmNpYWxfcGhyYXNlYmFuay8wZTFhMDZjNDkwMGZkYWU0NjA5MWQwMzEwNjg2MDFlMzc3M2JhMDY3YzdjZWNiNWIwZGExZGNiYTVjZTk4OWE2P3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&Signature=bAa5H2CkZQBChP22UPRRjLfw8JJKGCyyPG%7EAOWS9%7E9hmGV8Ma-ZA4mweWXZuqiu92if2vRtmdCCQsr4QzKnf6%7EPQqzBY1JrDIZAyioXT4HfGJvUI%7EA65E0adU5prmgy3s-eGKVbzVsjiNw97qDeM%7E-bntK8ZbxWIhIc7ksKNFS-T-wjxTYzWtQ1yW7LmdylUfq1JFS3jG-kxueKoRYeL0ZJXBysES5mefIbRdkoMrNpAZ-%7EMApmTArzg7pHO48YDrftQiSo346YS-2GMJJlQl7P9GPCmRP48L2A9DH8k0k48q90ILEEfOAuE7NIhOeZuTwhFalvN5Jmj0MVdqwG-bQ__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
      "--2024-04-25 20:54:49--  https://cdn-lfs.hf-mirror.com/datasets/financial_phrasebank/0e1a06c4900fdae46091d031068601e3773ba067c7cecb5b0da1dcba5ce989a6?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27FinancialPhraseBank-v1.0.zip%3B+filename%3D%22FinancialPhraseBank-v1.0.zip%22%3B&response-content-type=application%2Fzip&Expires=1714308541&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxNDMwODU0MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9kYXRhc2V0cy9maW5hbmNpYWxfcGhyYXNlYmFuay8wZTFhMDZjNDkwMGZkYWU0NjA5MWQwMzEwNjg2MDFlMzc3M2JhMDY3YzdjZWNiNWIwZGExZGNiYTVjZTk4OWE2P3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&Signature=bAa5H2CkZQBChP22UPRRjLfw8JJKGCyyPG%7EAOWS9%7E9hmGV8Ma-ZA4mweWXZuqiu92if2vRtmdCCQsr4QzKnf6%7EPQqzBY1JrDIZAyioXT4HfGJvUI%7EA65E0adU5prmgy3s-eGKVbzVsjiNw97qDeM%7E-bntK8ZbxWIhIc7ksKNFS-T-wjxTYzWtQ1yW7LmdylUfq1JFS3jG-kxueKoRYeL0ZJXBysES5mefIbRdkoMrNpAZ-%7EMApmTArzg7pHO48YDrftQiSo346YS-2GMJJlQl7P9GPCmRP48L2A9DH8k0k48q90ILEEfOAuE7NIhOeZuTwhFalvN5Jmj0MVdqwG-bQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
      "Resolving cdn-lfs.hf-mirror.com (cdn-lfs.hf-mirror.com)... 162.159.33.143, 104.21.63.170, 162.159.142.99, ...\n",
      "Connecting to cdn-lfs.hf-mirror.com (cdn-lfs.hf-mirror.com)|162.159.33.143|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 681890 (666K) [application/zip]\n",
      "Saving to: ‘FinancialPhraseBank-v1.0.zip.1’\n",
      "\n",
      "FinancialPhraseBank 100%[===================>] 665.91K  1.49MB/s    in 0.4s    \n",
      "\n",
      "2024-04-25 20:54:50 (1.49 MB/s) - ‘FinancialPhraseBank-v1.0.zip.1’ saved [681890/681890]\n",
      "\n",
      "Archive:  FinancialPhraseBank-v1.0.zip\n",
      "   creating: FinancialPhraseBank-v1.0/\n",
      "  inflating: FinancialPhraseBank-v1.0/License.txt  \n",
      "   creating: __MACOSX/\n",
      "   creating: __MACOSX/FinancialPhraseBank-v1.0/\n",
      "  inflating: __MACOSX/FinancialPhraseBank-v1.0/._License.txt  \n",
      "  inflating: FinancialPhraseBank-v1.0/README.txt  \n",
      "  inflating: __MACOSX/FinancialPhraseBank-v1.0/._README.txt  \n",
      "  inflating: FinancialPhraseBank-v1.0/Sentences_50Agree.txt  \n",
      "  inflating: FinancialPhraseBank-v1.0/Sentences_66Agree.txt  \n",
      "  inflating: FinancialPhraseBank-v1.0/Sentences_75Agree.txt  \n",
      "  inflating: FinancialPhraseBank-v1.0/Sentences_AllAgree.txt  \n"
     ]
    }
   ],
   "source": [
    "# download dataset\n",
    "!wget https://hf-mirror.com/datasets/financial_phrasebank/resolve/main/data/FinancialPhraseBank-v1.0.zip\n",
    "!unzip FinancialPhraseBank-v1.0.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(source,   batch_size=32, shuffle=False):\n",
    "\n",
    "    column_names = ['input_ids', 'attention_mask','labels','text_labels']\n",
    "    \n",
    "    dataset = GeneratorDataset(source, column_names=column_names, shuffle=shuffle)\n",
    "    # transforms\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "class MSDataset:\n",
    "    def __init__(self, filepath,tokenizer,max_length):\n",
    "        self.path = filepath\n",
    "        self.sentences = []\n",
    "        self.labels = []\n",
    "        self.text_labels = []\n",
    "        self._load()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def _load(self):\n",
    "        label_mapping = {\n",
    "            \"negative\": 0,\n",
    "            \"neutral\": 1,\n",
    "            \"positive\": 2\n",
    "        }\n",
    "        with open(self.path, encoding=\"iso-8859-1\") as f:\n",
    "            for line in f:\n",
    "                sentence, label_text = line.strip().split(\"@\")\n",
    "                self.sentences.append(sentence)\n",
    "                self.labels.append(label_mapping[label_text])\n",
    "                self.text_labels.append(label_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.sentences[index]\n",
    "        text_labels = self.text_labels[index]\n",
    "        model_inputs = self.tokenizer(sentence, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"np\")\n",
    "        labels = self.tokenizer(text_labels, max_length=3, padding=\"max_length\", truncation=True, return_tensors=\"np\")\n",
    "        labels = labels[\"input_ids\"]\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return model_inputs['input_ids'], model_inputs['attention_mask'], labels,self.text_labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "dataset = process_dataset(MSDataset(\"./FinancialPhraseBank-v1.0/Sentences_AllAgree.txt\",tokenizer,max_length),batch_size=batch_size)\n",
    "\n",
    "train_dataset, eval_dataset = dataset.split([0.9, 0.1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = mindspore.nn.AdamWeightDecay(model.trainable_params(), learning_rate=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': Tensor(shape=[8, 1, 128], dtype=Int64, value=\n",
      "[[[    0, 46583,    16 ...     1,     1,     1]],\n",
      " [[    0,   717,  5160 ...     1,     1,     1]],\n",
      " [[    0,  9682,   438 ...     1,     1,     1]],\n",
      " ...\n",
      " [[    0, 27602, 28477 ...     1,     1,     1]],\n",
      " [[    0,   717,  2492 ...     1,     1,     1]],\n",
      " [[    0,  9089,   523 ...     1,     1,     1]]]), 'attention_mask': Tensor(shape=[8, 1, 128], dtype=Int64, value=\n",
      "[[[1, 1, 1 ... 0, 0, 0]],\n",
      " [[1, 1, 1 ... 0, 0, 0]],\n",
      " [[1, 1, 1 ... 0, 0, 0]],\n",
      " ...\n",
      " [[1, 1, 1 ... 0, 0, 0]],\n",
      " [[1, 1, 1 ... 0, 0, 0]],\n",
      " [[1, 1, 1 ... 0, 0, 0]]]), 'labels': Tensor(shape=[8, 1, 3], dtype=Int64, value=\n",
      "[[[    0, 12516,     2]],\n",
      " [[    0, 12516,     2]],\n",
      " [[    0, 12516,     2]],\n",
      " ...\n",
      " [[    0, 12516,     2]],\n",
      " [[    0, 12516,     2]],\n",
      " [[    0, 12516,     2]]]), 'text_labels': Tensor(shape=[8], dtype=String, value= ['neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral',\n",
      " 'neutral', 'neutral'])}\n"
     ]
    }
   ],
   "source": [
    "print(next(train_dataset.create_dict_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/255 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model.base_model.peft_config[\"default\"].total_step = len(train_dataset) * num_epochs\n",
    "from mindspore import Tensor\n",
    "\n",
    "num_batches = len(train_dataset)\n",
    "num_batches_eval = len(eval_dataset)\n",
    "                       \n",
    "def forward_fn(input_ids,attention_mask,labels ):\n",
    "        output = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "        return output.loss, output.logits\n",
    "    \n",
    "grad_fn = mindspore.value_and_grad(\n",
    "        forward_fn, None,model.trainable_params(), has_aux=True,return_ids=True\n",
    "    )\n",
    "global_step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.set_train(True)\n",
    "    total_loss, total_step = 0, 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with tqdm(total=num_batches) as t:\n",
    "        for step, (input_ids,attention_mask,labels,_) in enumerate(train_dataset):\n",
    "            input_ids  = input_ids.squeeze(axis=1)\n",
    "            labels  = labels.squeeze(axis=1)\n",
    "            attention_mask = attention_mask.squeeze(axis=1)\n",
    "            (loss, logits), grad = grad_fn(input_ids,attention_mask,labels)\n",
    "            gradient = [g for _, g in grad]\n",
    "            gradient = tuple(gradient)\n",
    "            optimizer(gradient)\n",
    "            model.base_model.update_and_allocate(global_step, grad)\n",
    "            total_loss += loss.asnumpy()\n",
    "            total_step += 1\n",
    "            global_step += 1\n",
    "            curr_loss = total_loss / total_step\n",
    "            t.set_postfix({'train-loss': f'{curr_loss:.2f}'})\n",
    "            t.update(1)\n",
    "    model.set_train(False)\n",
    "    eval_loss = 0\n",
    "    total_step = 0\n",
    "    eval_preds = []\n",
    "    text_labels = []\n",
    "    with tqdm(total=num_batches_eval) as t:\n",
    "        for step, (input_ids,attention_mask,labels,text) in enumerate(eval_dataset):\n",
    "            input_ids = input_ids.squeeze(axis=1)\n",
    "            labels = labels.squeeze(axis=1)\n",
    "            attention_mask = attention_mask.squeeze(axis=1)\n",
    "            outputs = model(input_ids=input_ids,attention_mask=attention_mask,labels=labels)\n",
    "            loss = outputs.loss\n",
    "            eval_loss += loss.asnumpy()\n",
    "            total_step += 1           \n",
    "            eval_loss = total_loss / total_step\n",
    "            eval_preds.extend(\n",
    "                tokenizer.batch_decode(np.argmax(outputs.logits.asnumpy(), -1), skip_special_tokens=True)\n",
    "            )\n",
    "            text_str = str(text.asnumpy())\n",
    "            text_str = text_str.replace(\"[\", \"\").replace(\"]\", \"\").replace(\",\", \"\").replace(\"'\", \"\")\n",
    "            labels = text_str.split(\" \")\n",
    "            text_labels.extend(labels)\n",
    "            t.set_postfix({'eval-loss': f'{eval_loss:.2f}'})\n",
    "            t.update(1)\n",
    "    for pred, text_label in zip(eval_preds, text_labels):\n",
    "        if pred.strip() == text_label.strip():\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"{accuracy=} % on the evaluation dataset\")\n",
    "    eval_epoch_loss = eval_loss / eval_dataset.get_dataset_size()\n",
    "    eval_ppl = np.exp(eval_epoch_loss)\n",
    "    train_epoch_loss = total_loss / train_dataset.get_dataset_size()\n",
    "    train_ppl = np.exp(train_epoch_loss)\n",
    "    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_id = f\"{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\"\n",
    "model.save_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = f\"{peft_model_id}/adapter_model.ckpt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(857257:140373268870336,MainProcess):2024-04-25-18:53:07.740.385 [mindspore/train/serialization.py:1456] For 'load_param_into_net', 355 parameters in the 'net' are not loaded, because they are not in the 'parameter_dict', please check whether the network structure is consistent when training and loading checkpoint.\n",
      "[WARNING] ME(857257:140373268870336,MainProcess):2024-04-25-18:53:07.741.240 [mindspore/train/serialization.py:1460] ['base_model.model.shared.weight', 'base_model.model.encoder.embed_positions.weight', 'base_model.model.encoder.layers.0.self_attn.k_proj.weight', 'base_model.model.encoder.layers.0.self_attn.k_proj.bias', 'base_model.model.model.encoder.layers.0.self_attn.k_proj.ranknum.default', 'base_model.model.encoder.layers.0.self_attn.v_proj.weight', 'base_model.model.encoder.layers.0.self_attn.v_proj.bias', 'base_model.model.model.encoder.layers.0.self_attn.v_proj.ranknum.default', 'base_model.model.encoder.layers.0.self_attn.q_proj.weight', 'base_model.model.encoder.layers.0.self_attn.q_proj.bias', 'base_model.model.model.encoder.layers.0.self_attn.q_proj.ranknum.default', 'base_model.model.encoder.layers.0.self_attn.out_proj.weight', 'base_model.model.encoder.layers.0.self_attn.out_proj.bias', 'base_model.model.model.encoder.layers.0.self_attn.out_proj.ranknum.default', 'base_model.model.encoder.layers.0.self_attn_layer_norm.weight', 'base_model.model.encoder.layers.0.self_attn_layer_norm.bias', 'base_model.model.encoder.layers.0.fc1.weight', 'base_model.model.encoder.layers.0.fc1.bias', 'base_model.model.model.encoder.layers.0.fc1.ranknum.default', 'base_model.model.encoder.layers.0.fc2.weight', 'base_model.model.encoder.layers.0.fc2.bias', 'base_model.model.model.encoder.layers.0.fc2.ranknum.default', 'base_model.model.encoder.layers.0.final_layer_norm.weight', 'base_model.model.encoder.layers.0.final_layer_norm.bias', 'base_model.model.encoder.layers.1.self_attn.k_proj.weight', 'base_model.model.encoder.layers.1.self_attn.k_proj.bias', 'base_model.model.model.encoder.layers.1.self_attn.k_proj.ranknum.default', 'base_model.model.encoder.layers.1.self_attn.v_proj.weight', 'base_model.model.encoder.layers.1.self_attn.v_proj.bias', 'base_model.model.model.encoder.layers.1.self_attn.v_proj.ranknum.default', 'base_model.model.encoder.layers.1.self_attn.q_proj.weight', 'base_model.model.encoder.layers.1.self_attn.q_proj.bias', 'base_model.model.model.encoder.layers.1.self_attn.q_proj.ranknum.default', 'base_model.model.encoder.layers.1.self_attn.out_proj.weight', 'base_model.model.encoder.layers.1.self_attn.out_proj.bias', 'base_model.model.model.encoder.layers.1.self_attn.out_proj.ranknum.default', 'base_model.model.encoder.layers.1.self_attn_layer_norm.weight', 'base_model.model.encoder.layers.1.self_attn_layer_norm.bias', 'base_model.model.encoder.layers.1.fc1.weight', 'base_model.model.encoder.layers.1.fc1.bias', 'base_model.model.model.encoder.layers.1.fc1.ranknum.default', 'base_model.model.encoder.layers.1.fc2.weight', 'base_model.model.encoder.layers.1.fc2.bias', 'base_model.model.model.encoder.layers.1.fc2.ranknum.default', 'base_model.model.encoder.layers.1.final_layer_norm.weight', 'base_model.model.encoder.layers.1.final_layer_norm.bias', 'base_model.model.encoder.layers.2.self_attn.k_proj.weight', 'base_model.model.encoder.layers.2.self_attn.k_proj.bias', 'base_model.model.model.encoder.layers.2.self_attn.k_proj.ranknum.default', 'base_model.model.encoder.layers.2.self_attn.v_proj.weight', 'base_model.model.encoder.layers.2.self_attn.v_proj.bias', 'base_model.model.model.encoder.layers.2.self_attn.v_proj.ranknum.default', 'base_model.model.encoder.layers.2.self_attn.q_proj.weight', 'base_model.model.encoder.layers.2.self_attn.q_proj.bias', 'base_model.model.model.encoder.layers.2.self_attn.q_proj.ranknum.default', 'base_model.model.encoder.layers.2.self_attn.out_proj.weight', 'base_model.model.encoder.layers.2.self_attn.out_proj.bias', 'base_model.model.model.encoder.layers.2.self_attn.out_proj.ranknum.default', 'base_model.model.encoder.layers.2.self_attn_layer_norm.weight', 'base_model.model.encoder.layers.2.self_attn_layer_norm.bias', 'base_model.model.encoder.layers.2.fc1.weight', 'base_model.model.encoder.layers.2.fc1.bias', 'base_model.model.model.encoder.layers.2.fc1.ranknum.default', 'base_model.model.encoder.layers.2.fc2.weight', 'base_model.model.encoder.layers.2.fc2.bias', 'base_model.model.model.encoder.layers.2.fc2.ranknum.default', 'base_model.model.encoder.layers.2.final_layer_norm.weight', 'base_model.model.encoder.layers.2.final_layer_norm.bias', 'base_model.model.encoder.layers.3.self_attn.k_proj.weight', 'base_model.model.encoder.layers.3.self_attn.k_proj.bias', 'base_model.model.model.encoder.layers.3.self_attn.k_proj.ranknum.default', 'base_model.model.encoder.layers.3.self_attn.v_proj.weight', 'base_model.model.encoder.layers.3.self_attn.v_proj.bias', 'base_model.model.model.encoder.layers.3.self_attn.v_proj.ranknum.default', 'base_model.model.encoder.layers.3.self_attn.q_proj.weight', 'base_model.model.encoder.layers.3.self_attn.q_proj.bias', 'base_model.model.model.encoder.layers.3.self_attn.q_proj.ranknum.default', 'base_model.model.encoder.layers.3.self_attn.out_proj.weight', 'base_model.model.encoder.layers.3.self_attn.out_proj.bias', 'base_model.model.model.encoder.layers.3.self_attn.out_proj.ranknum.default', 'base_model.model.encoder.layers.3.self_attn_layer_norm.weight', 'base_model.model.encoder.layers.3.self_attn_layer_norm.bias', 'base_model.model.encoder.layers.3.fc1.weight', 'base_model.model.encoder.layers.3.fc1.bias', 'base_model.model.model.encoder.layers.3.fc1.ranknum.default', 'base_model.model.encoder.layers.3.fc2.weight', 'base_model.model.encoder.layers.3.fc2.bias', 'base_model.model.model.encoder.layers.3.fc2.ranknum.default', 'base_model.model.encoder.layers.3.final_layer_norm.weight', 'base_model.model.encoder.layers.3.final_layer_norm.bias', 'base_model.model.encoder.layers.4.self_attn.k_proj.weight', 'base_model.model.encoder.layers.4.self_attn.k_proj.bias', 'base_model.model.model.encoder.layers.4.self_attn.k_proj.ranknum.default', 'base_model.model.encoder.layers.4.self_attn.v_proj.weight', 'base_model.model.encoder.layers.4.self_attn.v_proj.bias', 'base_model.model.model.encoder.layers.4.self_attn.v_proj.ranknum.default', 'base_model.model.encoder.layers.4.self_attn.q_proj.weight', 'base_model.model.encoder.layers.4.self_attn.q_proj.bias', 'base_model.model.model.encoder.layers.4.self_attn.q_proj.ranknum.default', 'base_model.model.encoder.layers.4.self_attn.out_proj.weight', 'base_model.model.encoder.layers.4.self_attn.out_proj.bias', 'base_model.model.model.encoder.layers.4.self_attn.out_proj.ranknum.default', 'base_model.model.encoder.layers.4.self_attn_layer_norm.weight', 'base_model.model.encoder.layers.4.self_attn_layer_norm.bias', 'base_model.model.encoder.layers.4.fc1.weight', 'base_model.model.encoder.layers.4.fc1.bias', 'base_model.model.model.encoder.layers.4.fc1.ranknum.default', 'base_model.model.encoder.layers.4.fc2.weight', 'base_model.model.encoder.layers.4.fc2.bias', 'base_model.model.model.encoder.layers.4.fc2.ranknum.default', 'base_model.model.encoder.layers.4.final_layer_norm.weight', 'base_model.model.encoder.layers.4.final_layer_norm.bias', 'base_model.model.encoder.layers.5.self_attn.k_proj.weight', 'base_model.model.encoder.layers.5.self_attn.k_proj.bias', 'base_model.model.model.encoder.layers.5.self_attn.k_proj.ranknum.default', 'base_model.model.encoder.layers.5.self_attn.v_proj.weight', 'base_model.model.encoder.layers.5.self_attn.v_proj.bias', 'base_model.model.model.encoder.layers.5.self_attn.v_proj.ranknum.default', 'base_model.model.encoder.layers.5.self_attn.q_proj.weight', 'base_model.model.encoder.layers.5.self_attn.q_proj.bias', 'base_model.model.model.encoder.layers.5.self_attn.q_proj.ranknum.default', 'base_model.model.encoder.layers.5.self_attn.out_proj.weight', 'base_model.model.encoder.layers.5.self_attn.out_proj.bias', 'base_model.model.model.encoder.layers.5.self_attn.out_proj.ranknum.default', 'base_model.model.encoder.layers.5.self_attn_layer_norm.weight', 'base_model.model.encoder.layers.5.self_attn_layer_norm.bias', 'base_model.model.encoder.layers.5.fc1.weight', 'base_model.model.encoder.layers.5.fc1.bias', 'base_model.model.model.encoder.layers.5.fc1.ranknum.default', 'base_model.model.encoder.layers.5.fc2.weight', 'base_model.model.encoder.layers.5.fc2.bias', 'base_model.model.model.encoder.layers.5.fc2.ranknum.default', 'base_model.model.encoder.layers.5.final_layer_norm.weight', 'base_model.model.encoder.layers.5.final_layer_norm.bias', 'base_model.model.encoder.layernorm_embedding.weight', 'base_model.model.encoder.layernorm_embedding.bias', 'base_model.model.decoder.embed_positions.weight', 'base_model.model.decoder.layers.0.self_attn.k_proj.weight', 'base_model.model.decoder.layers.0.self_attn.k_proj.bias', 'base_model.model.model.decoder.layers.0.self_attn.k_proj.ranknum.default', 'base_model.model.decoder.layers.0.self_attn.v_proj.weight', 'base_model.model.decoder.layers.0.self_attn.v_proj.bias', 'base_model.model.model.decoder.layers.0.self_attn.v_proj.ranknum.default', 'base_model.model.decoder.layers.0.self_attn.q_proj.weight', 'base_model.model.decoder.layers.0.self_attn.q_proj.bias', 'base_model.model.model.decoder.layers.0.self_attn.q_proj.ranknum.default', 'base_model.model.decoder.layers.0.self_attn.out_proj.weight', 'base_model.model.decoder.layers.0.self_attn.out_proj.bias', 'base_model.model.model.decoder.layers.0.self_attn.out_proj.ranknum.default', 'base_model.model.decoder.layers.0.self_attn_layer_norm.weight', 'base_model.model.decoder.layers.0.self_attn_layer_norm.bias', 'base_model.model.decoder.layers.0.encoder_attn.k_proj.weight', 'base_model.model.decoder.layers.0.encoder_attn.k_proj.bias', 'base_model.model.model.decoder.layers.0.encoder_attn.k_proj.ranknum.default', 'base_model.model.decoder.layers.0.encoder_attn.v_proj.weight', 'base_model.model.decoder.layers.0.encoder_attn.v_proj.bias', 'base_model.model.model.decoder.layers.0.encoder_attn.v_proj.ranknum.default', 'base_model.model.decoder.layers.0.encoder_attn.q_proj.weight', 'base_model.model.decoder.layers.0.encoder_attn.q_proj.bias', 'base_model.model.model.decoder.layers.0.encoder_attn.q_proj.ranknum.default', 'base_model.model.decoder.layers.0.encoder_attn.out_proj.weight', 'base_model.model.decoder.layers.0.encoder_attn.out_proj.bias', 'base_model.model.model.decoder.layers.0.encoder_attn.out_proj.ranknum.default', 'base_model.model.decoder.layers.0.encoder_attn_layer_norm.weight', 'base_model.model.decoder.layers.0.encoder_attn_layer_norm.bias', 'base_model.model.decoder.layers.0.fc1.weight', 'base_model.model.decoder.layers.0.fc1.bias', 'base_model.model.model.decoder.layers.0.fc1.ranknum.default', 'base_model.model.decoder.layers.0.fc2.weight', 'base_model.model.decoder.layers.0.fc2.bias', 'base_model.model.model.decoder.layers.0.fc2.ranknum.default', 'base_model.model.decoder.layers.0.final_layer_norm.weight', 'base_model.model.decoder.layers.0.final_layer_norm.bias', 'base_model.model.decoder.layers.1.self_attn.k_proj.weight', 'base_model.model.decoder.layers.1.self_attn.k_proj.bias', 'base_model.model.model.decoder.layers.1.self_attn.k_proj.ranknum.default', 'base_model.model.decoder.layers.1.self_attn.v_proj.weight', 'base_model.model.decoder.layers.1.self_attn.v_proj.bias', 'base_model.model.model.decoder.layers.1.self_attn.v_proj.ranknum.default', 'base_model.model.decoder.layers.1.self_attn.q_proj.weight', 'base_model.model.decoder.layers.1.self_attn.q_proj.bias', 'base_model.model.model.decoder.layers.1.self_attn.q_proj.ranknum.default', 'base_model.model.decoder.layers.1.self_attn.out_proj.weight', 'base_model.model.decoder.layers.1.self_attn.out_proj.bias', 'base_model.model.model.decoder.layers.1.self_attn.out_proj.ranknum.default', 'base_model.model.decoder.layers.1.self_attn_layer_norm.weight', 'base_model.model.decoder.layers.1.self_attn_layer_norm.bias', 'base_model.model.decoder.layers.1.encoder_attn.k_proj.weight', 'base_model.model.decoder.layers.1.encoder_attn.k_proj.bias', 'base_model.model.model.decoder.layers.1.encoder_attn.k_proj.ranknum.default', 'base_model.model.decoder.layers.1.encoder_attn.v_proj.weight', 'base_model.model.decoder.layers.1.encoder_attn.v_proj.bias', 'base_model.model.model.decoder.layers.1.encoder_attn.v_proj.ranknum.default', 'base_model.model.decoder.layers.1.encoder_attn.q_proj.weight', 'base_model.model.decoder.layers.1.encoder_attn.q_proj.bias', 'base_model.model.model.decoder.layers.1.encoder_attn.q_proj.ranknum.default', 'base_model.model.decoder.layers.1.encoder_attn.out_proj.weight', 'base_model.model.decoder.layers.1.encoder_attn.out_proj.bias', 'base_model.model.model.decoder.layers.1.encoder_attn.out_proj.ranknum.default', 'base_model.model.decoder.layers.1.encoder_attn_layer_norm.weight', 'base_model.model.decoder.layers.1.encoder_attn_layer_norm.bias', 'base_model.model.decoder.layers.1.fc1.weight', 'base_model.model.decoder.layers.1.fc1.bias', 'base_model.model.model.decoder.layers.1.fc1.ranknum.default', 'base_model.model.decoder.layers.1.fc2.weight', 'base_model.model.decoder.layers.1.fc2.bias', 'base_model.model.model.decoder.layers.1.fc2.ranknum.default', 'base_model.model.decoder.layers.1.final_layer_norm.weight', 'base_model.model.decoder.layers.1.final_layer_norm.bias', 'base_model.model.decoder.layers.2.self_attn.k_proj.weight', 'base_model.model.decoder.layers.2.self_attn.k_proj.bias', 'base_model.model.model.decoder.layers.2.self_attn.k_proj.ranknum.default', 'base_model.model.decoder.layers.2.self_attn.v_proj.weight', 'base_model.model.decoder.layers.2.self_attn.v_proj.bias', 'base_model.model.model.decoder.layers.2.self_attn.v_proj.ranknum.default', 'base_model.model.decoder.layers.2.self_attn.q_proj.weight', 'base_model.model.decoder.layers.2.self_attn.q_proj.bias', 'base_model.model.model.decoder.layers.2.self_attn.q_proj.ranknum.default', 'base_model.model.decoder.layers.2.self_attn.out_proj.weight', 'base_model.model.decoder.layers.2.self_attn.out_proj.bias', 'base_model.model.model.decoder.layers.2.self_attn.out_proj.ranknum.default', 'base_model.model.decoder.layers.2.self_attn_layer_norm.weight', 'base_model.model.decoder.layers.2.self_attn_layer_norm.bias', 'base_model.model.decoder.layers.2.encoder_attn.k_proj.weight', 'base_model.model.decoder.layers.2.encoder_attn.k_proj.bias', 'base_model.model.model.decoder.layers.2.encoder_attn.k_proj.ranknum.default', 'base_model.model.decoder.layers.2.encoder_attn.v_proj.weight', 'base_model.model.decoder.layers.2.encoder_attn.v_proj.bias', 'base_model.model.model.decoder.layers.2.encoder_attn.v_proj.ranknum.default', 'base_model.model.decoder.layers.2.encoder_attn.q_proj.weight', 'base_model.model.decoder.layers.2.encoder_attn.q_proj.bias', 'base_model.model.model.decoder.layers.2.encoder_attn.q_proj.ranknum.default', 'base_model.model.decoder.layers.2.encoder_attn.out_proj.weight', 'base_model.model.decoder.layers.2.encoder_attn.out_proj.bias', 'base_model.model.model.decoder.layers.2.encoder_attn.out_proj.ranknum.default', 'base_model.model.decoder.layers.2.encoder_attn_layer_norm.weight', 'base_model.model.decoder.layers.2.encoder_attn_layer_norm.bias', 'base_model.model.decoder.layers.2.fc1.weight', 'base_model.model.decoder.layers.2.fc1.bias', 'base_model.model.model.decoder.layers.2.fc1.ranknum.default', 'base_model.model.decoder.layers.2.fc2.weight', 'base_model.model.decoder.layers.2.fc2.bias', 'base_model.model.model.decoder.layers.2.fc2.ranknum.default', 'base_model.model.decoder.layers.2.final_layer_norm.weight', 'base_model.model.decoder.layers.2.final_layer_norm.bias', 'base_model.model.decoder.layers.3.self_attn.k_proj.weight', 'base_model.model.decoder.layers.3.self_attn.k_proj.bias', 'base_model.model.model.decoder.layers.3.self_attn.k_proj.ranknum.default', 'base_model.model.decoder.layers.3.self_attn.v_proj.weight', 'base_model.model.decoder.layers.3.self_attn.v_proj.bias', 'base_model.model.model.decoder.layers.3.self_attn.v_proj.ranknum.default', 'base_model.model.decoder.layers.3.self_attn.q_proj.weight', 'base_model.model.decoder.layers.3.self_attn.q_proj.bias', 'base_model.model.model.decoder.layers.3.self_attn.q_proj.ranknum.default', 'base_model.model.decoder.layers.3.self_attn.out_proj.weight', 'base_model.model.decoder.layers.3.self_attn.out_proj.bias', 'base_model.model.model.decoder.layers.3.self_attn.out_proj.ranknum.default', 'base_model.model.decoder.layers.3.self_attn_layer_norm.weight', 'base_model.model.decoder.layers.3.self_attn_layer_norm.bias', 'base_model.model.decoder.layers.3.encoder_attn.k_proj.weight', 'base_model.model.decoder.layers.3.encoder_attn.k_proj.bias', 'base_model.model.model.decoder.layers.3.encoder_attn.k_proj.ranknum.default', 'base_model.model.decoder.layers.3.encoder_attn.v_proj.weight', 'base_model.model.decoder.layers.3.encoder_attn.v_proj.bias', 'base_model.model.model.decoder.layers.3.encoder_attn.v_proj.ranknum.default', 'base_model.model.decoder.layers.3.encoder_attn.q_proj.weight', 'base_model.model.decoder.layers.3.encoder_attn.q_proj.bias', 'base_model.model.model.decoder.layers.3.encoder_attn.q_proj.ranknum.default', 'base_model.model.decoder.layers.3.encoder_attn.out_proj.weight', 'base_model.model.decoder.layers.3.encoder_attn.out_proj.bias', 'base_model.model.model.decoder.layers.3.encoder_attn.out_proj.ranknum.default', 'base_model.model.decoder.layers.3.encoder_attn_layer_norm.weight', 'base_model.model.decoder.layers.3.encoder_attn_layer_norm.bias', 'base_model.model.decoder.layers.3.fc1.weight', 'base_model.model.decoder.layers.3.fc1.bias', 'base_model.model.model.decoder.layers.3.fc1.ranknum.default', 'base_model.model.decoder.layers.3.fc2.weight', 'base_model.model.decoder.layers.3.fc2.bias', 'base_model.model.model.decoder.layers.3.fc2.ranknum.default', 'base_model.model.decoder.layers.3.final_layer_norm.weight', 'base_model.model.decoder.layers.3.final_layer_norm.bias', 'base_model.model.decoder.layers.4.self_attn.k_proj.weight', 'base_model.model.decoder.layers.4.self_attn.k_proj.bias', 'base_model.model.model.decoder.layers.4.self_attn.k_proj.ranknum.default', 'base_model.model.decoder.layers.4.self_attn.v_proj.weight', 'base_model.model.decoder.layers.4.self_attn.v_proj.bias', 'base_model.model.model.decoder.layers.4.self_attn.v_proj.ranknum.default', 'base_model.model.decoder.layers.4.self_attn.q_proj.weight', 'base_model.model.decoder.layers.4.self_attn.q_proj.bias', 'base_model.model.model.decoder.layers.4.self_attn.q_proj.ranknum.default', 'base_model.model.decoder.layers.4.self_attn.out_proj.weight', 'base_model.model.decoder.layers.4.self_attn.out_proj.bias', 'base_model.model.model.decoder.layers.4.self_attn.out_proj.ranknum.default', 'base_model.model.decoder.layers.4.self_attn_layer_norm.weight', 'base_model.model.decoder.layers.4.self_attn_layer_norm.bias', 'base_model.model.decoder.layers.4.encoder_attn.k_proj.weight', 'base_model.model.decoder.layers.4.encoder_attn.k_proj.bias', 'base_model.model.model.decoder.layers.4.encoder_attn.k_proj.ranknum.default', 'base_model.model.decoder.layers.4.encoder_attn.v_proj.weight', 'base_model.model.decoder.layers.4.encoder_attn.v_proj.bias', 'base_model.model.model.decoder.layers.4.encoder_attn.v_proj.ranknum.default', 'base_model.model.decoder.layers.4.encoder_attn.q_proj.weight', 'base_model.model.decoder.layers.4.encoder_attn.q_proj.bias', 'base_model.model.model.decoder.layers.4.encoder_attn.q_proj.ranknum.default', 'base_model.model.decoder.layers.4.encoder_attn.out_proj.weight', 'base_model.model.decoder.layers.4.encoder_attn.out_proj.bias', 'base_model.model.model.decoder.layers.4.encoder_attn.out_proj.ranknum.default', 'base_model.model.decoder.layers.4.encoder_attn_layer_norm.weight', 'base_model.model.decoder.layers.4.encoder_attn_layer_norm.bias', 'base_model.model.decoder.layers.4.fc1.weight', 'base_model.model.decoder.layers.4.fc1.bias', 'base_model.model.model.decoder.layers.4.fc1.ranknum.default', 'base_model.model.decoder.layers.4.fc2.weight', 'base_model.model.decoder.layers.4.fc2.bias', 'base_model.model.model.decoder.layers.4.fc2.ranknum.default', 'base_model.model.decoder.layers.4.final_layer_norm.weight', 'base_model.model.decoder.layers.4.final_layer_norm.bias', 'base_model.model.decoder.layers.5.self_attn.k_proj.weight', 'base_model.model.decoder.layers.5.self_attn.k_proj.bias', 'base_model.model.model.decoder.layers.5.self_attn.k_proj.ranknum.default', 'base_model.model.decoder.layers.5.self_attn.v_proj.weight', 'base_model.model.decoder.layers.5.self_attn.v_proj.bias', 'base_model.model.model.decoder.layers.5.self_attn.v_proj.ranknum.default', 'base_model.model.decoder.layers.5.self_attn.q_proj.weight', 'base_model.model.decoder.layers.5.self_attn.q_proj.bias', 'base_model.model.model.decoder.layers.5.self_attn.q_proj.ranknum.default', 'base_model.model.decoder.layers.5.self_attn.out_proj.weight', 'base_model.model.decoder.layers.5.self_attn.out_proj.bias', 'base_model.model.model.decoder.layers.5.self_attn.out_proj.ranknum.default', 'base_model.model.decoder.layers.5.self_attn_layer_norm.weight', 'base_model.model.decoder.layers.5.self_attn_layer_norm.bias', 'base_model.model.decoder.layers.5.encoder_attn.k_proj.weight', 'base_model.model.decoder.layers.5.encoder_attn.k_proj.bias', 'base_model.model.model.decoder.layers.5.encoder_attn.k_proj.ranknum.default', 'base_model.model.decoder.layers.5.encoder_attn.v_proj.weight', 'base_model.model.decoder.layers.5.encoder_attn.v_proj.bias', 'base_model.model.model.decoder.layers.5.encoder_attn.v_proj.ranknum.default', 'base_model.model.decoder.layers.5.encoder_attn.q_proj.weight', 'base_model.model.decoder.layers.5.encoder_attn.q_proj.bias', 'base_model.model.model.decoder.layers.5.encoder_attn.q_proj.ranknum.default', 'base_model.model.decoder.layers.5.encoder_attn.out_proj.weight', 'base_model.model.decoder.layers.5.encoder_attn.out_proj.bias', 'base_model.model.model.decoder.layers.5.encoder_attn.out_proj.ranknum.default', 'base_model.model.decoder.layers.5.encoder_attn_layer_norm.weight', 'base_model.model.decoder.layers.5.encoder_attn_layer_norm.bias', 'base_model.model.decoder.layers.5.fc1.weight', 'base_model.model.decoder.layers.5.fc1.bias', 'base_model.model.model.decoder.layers.5.fc1.ranknum.default', 'base_model.model.decoder.layers.5.fc2.weight', 'base_model.model.decoder.layers.5.fc2.bias', 'base_model.model.model.decoder.layers.5.fc2.ranknum.default', 'base_model.model.decoder.layers.5.final_layer_norm.weight', 'base_model.model.decoder.layers.5.final_layer_norm.bias', 'base_model.model.decoder.layernorm_embedding.weight', 'base_model.model.decoder.layernorm_embedding.bias'] are not loaded.\n"
     ]
    }
   ],
   "source": [
    "peft_model_id = f\"{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}\"\n",
    "\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lyk_ms2.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
