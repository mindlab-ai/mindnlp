{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mindspore\n",
    "import argparse\n",
    "import numpy as np\n",
    "import logging\n",
    "import mindspore.dataset as ds\n",
    "import os\n",
    "\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from mindspore.nn import CrossEntropyLoss\n",
    "from mindspore import nn, ops\n",
    "from mindspore.train.serialization import save_checkpoint\n",
    "from mindspore.dataset import TextFileDataset\n",
    "\n",
    "from mindnlp.transforms import BertTokenizer\n",
    "from mindnlp.modules import Accumulator\n",
    "from mindnlp.models import GPT2Config, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 8\n",
    "\n",
    "lr = 1e-4\n",
    "warmup_steps = 2000\n",
    "accumulate_step = 2\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "log_step = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mindnlp.utils import cached_file\n",
    "\n",
    "url = 'https://download.mindspore.cn/toolkits/mindnlp/dataset/text_generation/nlpcc2017/train_with_summ.txt'\n",
    "path, _ = cached_file('train_with_summ.txt', './', url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = TextFileDataset(str(path), shuffle=False)\n",
    "dataset.get_dataset_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = dataset.split([0.9, 0.1], randomize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# article: [CLS] xxxxx [SEP]\n",
    "# summary: [CLS] xxxxx [SEP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def process_dataset(dataset, tokenizer, batch_size=6, max_seq_len=1024, shuffle=False):\n",
    "    def read_map(text):\n",
    "        data = json.loads(text.tobytes())\n",
    "        return np.array(data['article']), np.array(data['summarization'])\n",
    "\n",
    "    def merge_and_pad(article, summary):\n",
    "        article_len = len(article)\n",
    "        summary_len = len(summary)\n",
    "\n",
    "        sep_id = np.array([tokenizer.sep_token_id])\n",
    "        pad_id = np.array([tokenizer.pad_token_id])\n",
    "        if article_len + summary_len > max_seq_len:\n",
    "            new_article_len = max_seq_len - summary_len\n",
    "            merged = np.concatenate([article[:new_article_len], sep_id, summary[1:]])\n",
    "        elif article_len + summary_len - 1 < max_seq_len:\n",
    "            pad_len = max_seq_len - article_len - summary_len + 1\n",
    "            pad_text = np.array([tokenizer.pad_token_id] * pad_len)\n",
    "            merged = np.concatenate([article, summary[1:], pad_text])\n",
    "        else:\n",
    "            merged = np.concatenate([article, summary[1:]])\n",
    "            \n",
    "        return merged.astype(np.int32)\n",
    "\n",
    "    dataset = dataset.map(read_map, 'text', ['article', 'summary'])\n",
    "    dataset = dataset.map(tokenizer, 'article')\n",
    "    dataset = dataset.map(tokenizer, 'summary')\n",
    "    dataset = dataset.map(merge_and_pad, ['article', 'summary'], 'input_ids')\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(batch_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = process_dataset(train_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "next(train_dataset.create_tuple_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mindnlp._legacy.amp import auto_mixed_precision\n",
    "\n",
    "config = GPT2Config(vocab_size=len(tokenizer))\n",
    "model = GPT2LMHeadModel(config, ignore_index=tokenizer.pad_token_id)\n",
    "model = auto_mixed_precision(model, 'O1')\n",
    "\n",
    "optimizer = nn.AdamWeightDecay(model.trainable_params(), lr)\n",
    "accumulator = Accumulator(optimizer, accumulate_step, max_grad_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mindspore import ops, ms_function\n",
    "from mindnlp._legacy.amp import DynamicLossScaler, all_finite\n",
    "# Define forward function\n",
    "\n",
    "loss_scaler = DynamicLossScaler(scale_value=2**10, scale_factor=2, scale_window=50)\n",
    "\n",
    "def forward_fn(input_ids, labels):\n",
    "    outputs = model(input_ids, labels=labels)\n",
    "    loss = outputs[0]\n",
    "    return loss_scaler.scale(loss / accumulate_step)\n",
    "\n",
    "# Get gradient function\n",
    "grad_fn = ops.value_and_grad(forward_fn, None, model.trainable_params())\n",
    "\n",
    "# Define function of one-step training\n",
    "@ms_function\n",
    "def train_step(data, label):\n",
    "    loss, grads = grad_fn(data, label)\n",
    "    loss = loss_scaler.unscale(loss)\n",
    "\n",
    "    is_finite = all_finite(grads)\n",
    "    if is_finite:\n",
    "        grads = loss_scaler.unscale(grads)\n",
    "        loss = ops.depend(loss, accumulator(grads))\n",
    "    loss = ops.depend(loss, loss_scaler.adjust(is_finite))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 记录模型参数数量\n",
    "num_parameters = 0\n",
    "parameters = model.trainable_params()\n",
    "for parameter in parameters:\n",
    "    num_parameters += parameter.numel()\n",
    "print('number of model parameters: {}'.format(num_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "total = train_dataset.get_dataset_size()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    with tqdm(total=total) as progress:\n",
    "        progress.set_description(f'Epoch {epoch}')\n",
    "        loss_total = 0\n",
    "        cur_step_nums = 0\n",
    "        for batch_idx, (input_ids,) in enumerate(train_dataset.create_tuple_iterator()):\n",
    "            cur_step_nums += 1\n",
    "            loss = train_step(input_ids, input_ids)\n",
    "            loss_total += loss\n",
    "\n",
    "            progress.set_postfix(loss=loss_total/cur_step_nums)\n",
    "            progress.update(1)\n",
    "        save_checkpoint(model, f\"gpt2_summarization_epoch_{epoch}.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_test_dataset(dataset, tokenizer, batch_size=1, max_seq_len=1024, max_summary_len=100):\n",
    "    def read_map(text):\n",
    "        data = json.loads(text.tobytes())\n",
    "        return np.array(data['article']), np.array(data['summarization'])\n",
    "\n",
    "    def pad(article):\n",
    "        article_len = len(article)\n",
    "        max_article_len = max_seq_len - max_summary_len\n",
    "        if article_len >= max_article_len:\n",
    "            article = np.concatenate([article[:max_article_len-1], np.array([tokenizer.sep_token_id]), np.array([tokenizer.pad_token_id] * max_summary_len)])\n",
    "            return article, max_article_len\n",
    "        else:\n",
    "            pad_len = max_seq_len - len(article)\n",
    "            article = np.concatenate([article, np.array([tokenizer.pad_token_id] * pad_len)])\n",
    "            return article, article_len\n",
    "\n",
    "    dataset = dataset.map(read_map, 'text', ['article', 'summary'])\n",
    "    dataset = dataset.map(tokenizer, 'article')\n",
    "    dataset = dataset.map(pad, 'article', ['input_ids', 'article_len'])\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batched_test_dataset = process_test_dataset(test_dataset, tokenizer, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(next(batched_test_dataset.create_tuple_iterator(output_numpy=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mindspore import Tensor\n",
    "# [CLS] article [SEP] [PAD]\n",
    "# article [SEP] [PAD]\n",
    "def generate(input_ids, article_len, model, tokenizer, max_summary_len=100):\n",
    "    curr_idx = int(article_len)\n",
    "    for i in range(max_summary_len):\n",
    "        outputs = model(Tensor(input_ids))\n",
    "        output_id = outputs[0].asnumpy()[:,curr_idx - 1].argmax()\n",
    "\n",
    "        if output_id == tokenizer.sep_token_id:\n",
    "            break\n",
    "        input_ids[:, curr_idx] = output_id\n",
    "        curr_idx += 1\n",
    "    output_ids = input_ids[:, int(article_len):curr_idx]\n",
    "    return output_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.set_train(False)\n",
    "\n",
    "i = 0\n",
    "for (input_ids, article_len, raw_summary) in batched_test_dataset.create_tuple_iterator(output_numpy=True):\n",
    "    output_ids = generate(input_ids, article_len, model, tokenizer)\n",
    "    output_text = tokenizer.decode(output_ids[0].tolist())\n",
    "    print(output_text)\n",
    "    i += 1\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
