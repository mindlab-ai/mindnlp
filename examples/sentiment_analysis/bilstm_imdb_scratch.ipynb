{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f29588be-fbc2-4101-8734-661a716ea1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] ME(60206:140704969580736,MainProcess):2023-03-09-09:54:05.532.516 [mindspore/run_check/_check_version.py:226] Cuda ['10.1', '11.1', '11.6'] version(libcu*.so need by mindspore-gpu) is not found, please confirm that the path of cuda is set to the env LD_LIBRARY_PATH, or check whether the CUDA version in wheel package and the CUDA runtime in current device matches, please refer to the installation guidelines: https://www.mindspore.cn/install\n",
      "[ERROR] ME(60206:140704969580736,MainProcess):2023-03-09-09:54:05.566.355 [mindspore/run_check/_check_version.py:226] Cuda ['10.1', '11.1', '11.6'] version(libcudnn*.so need by mindspore-gpu) is not found, please confirm that the path of cuda is set to the env LD_LIBRARY_PATH, or check whether the CUDA version in wheel package and the CUDA runtime in current device matches, please refer to the installation guidelines: https://www.mindspore.cn/install\n"
     ]
    }
   ],
   "source": [
    "import mindspore\n",
    "from mindspore import nn\n",
    "from mindspore.common.initializer import initializer\n",
    "\n",
    "import mindnlp\n",
    "from mindnlp.modules import Glove\n",
    "from mindnlp.transforms import BasicTokenizer, PadTransform, Lookup\n",
    "from mindnlp.scoring.metrics import accuracy\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daab0038-dc5d-4b0f-ab3b-5e8793c7f5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train, imdb_test = mindnlp.load_dataset('imdb', split=['train', 'test'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "206e4277-af8e-430b-9533-1a612daf0a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:This parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "WARNING:root:This parameter `keep_prob` will be deprecated, please use `p` instead.\n"
     ]
    }
   ],
   "source": [
    "# load embedding and vocab\n",
    "embedding, vocab = Glove.from_pretrained('6B', 100, special_tokens=[\"<unk>\", \"<pad>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6caea8f-e16f-4df9-a275-d5cb51c4b817",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BasicTokenizer(lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35455298-838d-4e6d-9317-e0fd39e56241",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_op = Lookup(vocab, unknown_token='<unk>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4719b5b9-b413-47b1-b0df-9b357a5d6b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256\n",
    "pad_op = PadTransform(max_length, pad_value=vocab.tokens_to_ids('<pad>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bae9d35-ba6b-40d7-9b7d-6d91ee2c5f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train = imdb_train.map([tokenizer, lookup_op, pad_op], 'text')\n",
    "imdb_test = imdb_test.map([tokenizer, lookup_op, pad_op], 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74e2d623-3c78-4852-8dde-9429853ad4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "imdb_train = imdb_train.batch(batch_size)\n",
    "imdb_test = imdb_test.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "991d9198-51fb-4f75-85fc-296278d916ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(60206:140704969580736,MainProcess):2023-03-09-09:54:39.683.544 [mindspore/dataset/engine/datasets.py:1142] Dataset is shuffled before split.\n"
     ]
    }
   ],
   "source": [
    "imdb_train, imdb_valid = imdb_train.split([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f07b456-0277-4387-b438-7f7b1a083b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import mindspore as ms\n",
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "from mindspore.common.initializer import Uniform, HeUniform\n",
    "\n",
    "class RNN(nn.Cell):\n",
    "    def __init__(self, embedding, hidden_dim, output_dim, n_layers,\n",
    "                 bidirectional):\n",
    "        super().__init__()\n",
    "        embedding_dim = embedding._embed_dim\n",
    "        self.embedding = embedding\n",
    "        self.rnn = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional,\n",
    "                           batch_first=True,\n",
    "                           dropout=0.5)\n",
    "        self.fc = nn.Dense(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def construct(self, inputs):\n",
    "        embedded = self.embedding(inputs)\n",
    "        _, (hidden, _) = self.rnn(embedded)\n",
    "        hidden = ops.concat((hidden[-2, :, :], hidden[-1, :, :]), axis=1)\n",
    "        output = self.fc(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "551c0b0b-d116-4c72-825e-7d022506a2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "output_size = 2\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "lr = 5e-4\n",
    "\n",
    "model = RNN(embedding, hidden_size, output_size, num_layers, bidirectional)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = nn.Adam(model.trainable_params(), learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bea9aa5c-2dc0-455c-b780-1c011fa4d811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Dense):\n",
    "        m.weight.set_data(initializer('xavier_normal', m.weight.shape, m.weight.dtype))\n",
    "        m.bias.set_data(initializer('zeros', m.bias.shape, m.bias.dtype))\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for name, param in m.parameters_and_names():\n",
    "            if 'bias' in name:\n",
    "                param.set_data(initializer('zeros', param.shape, param.dtype))\n",
    "            elif 'weight' in name:\n",
    "                param.set_data(initializer('orthogonal', param.shape, param.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af269446-61cf-4093-9c44-11277f0ba6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:This parameter `keep_prob` will be deprecated, please use `p` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RNN<\n",
       "  (embedding): Glove<\n",
       "    (dropout_layer): Dropout<keep_prob=1.0>\n",
       "    >\n",
       "  (rnn): LSTM<\n",
       "    (rnn): _DynamicLSTMCPUGPU<>\n",
       "    (dropout_op): Dropout<p=0.5>\n",
       "    >\n",
       "  (fc): Dense<input_channels=512, output_channels=2, has_bias=True>\n",
       "  >"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66750264-9d22-4f47-aec1-8e49fb6db0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_fn(data, label):\n",
    "    logits = model(data)\n",
    "    loss = loss_fn(logits, label)\n",
    "    return loss\n",
    "\n",
    "grad_fn = ms.value_and_grad(forward_fn, None, optimizer.parameters)\n",
    "\n",
    "def train_step(data, label):\n",
    "    loss, grads = grad_fn(data, label)\n",
    "    optimizer(grads)\n",
    "    return loss\n",
    "\n",
    "def train_one_epoch(model, train_dataset, epoch=0):\n",
    "    model.set_train()\n",
    "    total = train_dataset.get_dataset_size()\n",
    "    loss_total = 0\n",
    "    step_total = 0\n",
    "    with tqdm(total=total) as t:\n",
    "        t.set_description('Epoch %i' % epoch)\n",
    "        for data, label in train_dataset.create_tuple_iterator():\n",
    "            loss = train_step(data, label.astype(mindspore.int32))\n",
    "            loss_total += loss.asnumpy()\n",
    "            step_total += 1\n",
    "            t.set_postfix(loss=loss_total/step_total)\n",
    "            t.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ab6c926-b5c9-4ddc-84cc-4282d24a6851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataset, criterion, epoch=0):\n",
    "    total = test_dataset.get_dataset_size()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    step_total = 0\n",
    "    model.set_train(False)\n",
    "\n",
    "    with tqdm(total=total) as t:\n",
    "        t.set_description('Epoch %i' % epoch)\n",
    "        for i in test_dataset.create_tuple_iterator():\n",
    "            predictions = model(i[0])\n",
    "            loss = criterion(predictions, i[1].astype(mindspore.int32))\n",
    "            epoch_loss += loss.asnumpy()\n",
    "\n",
    "            acc = accuracy(predictions, i[1])\n",
    "            epoch_acc += acc\n",
    "\n",
    "            step_total += 1\n",
    "            t.set_postfix(loss=epoch_loss/step_total, acc=epoch_acc/step_total)\n",
    "            t.update(1)\n",
    "\n",
    "    return epoch_loss / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63810da0-d866-4a9f-896f-d50441996831",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                                                                                     | 0/274 [00:00<?, ?it/s]Could not load symbol cublasGetSmCountTarget from libcublas.so.11. Error: /usr/local/cuda-11.1/lib64/libcublas.so.11: undefined symbol: cublasGetSmCountTarget\n",
      "Epoch 0: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 274/274 [00:34<00:00,  8.05it/s, loss=0.661]\n",
      "Epoch 0: 100%|████████████████████████████████████████████████████████████████████████████████████| 117/117 [00:07<00:00, 14.66it/s, acc=0.677, loss=0.598]\n",
      "Epoch 1: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 274/274 [00:29<00:00,  9.41it/s, loss=0.641]\n",
      "Epoch 1: 100%|█████████████████████████████████████████████████████████████████████████████████████| 117/117 [00:09<00:00, 13.00it/s, acc=0.606, loss=0.66]\n",
      "Epoch 2:   0%|                                                                                                                     | 0/274 [00:00<?, ?it/s][WARNING] PYNATIVE(60206,7ff86dbae0c0,python):2023-03-09-09:56:05.135.846 [mindspore/ccsrc/pipeline/pynative/grad/grad.cc:1198] CheckAlreadyRun] The input info of this cell has changed, forward process will run again\n",
      "Epoch 2: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 274/274 [00:24<00:00, 11.12it/s, loss=0.553]\n",
      "Epoch 2: 100%|████████████████████████████████████████████████████████████████████████████████████| 117/117 [00:08<00:00, 13.71it/s, acc=0.812, loss=0.428]\n",
      "Epoch 3:   0%|                                                                                                                     | 0/274 [00:00<?, ?it/s][WARNING] PYNATIVE(60206,7ff86dbae0c0,python):2023-03-09-09:56:38.948.665 [mindspore/ccsrc/pipeline/pynative/grad/grad.cc:1198] CheckAlreadyRun] The input info of this cell has changed, forward process will run again\n",
      "Epoch 3: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 274/274 [00:27<00:00,  9.81it/s, loss=0.447]\n",
      "Epoch 3: 100%|████████████████████████████████████████████████████████████████████████████████████| 117/117 [00:09<00:00, 12.87it/s, acc=0.847, loss=0.389]\n",
      "Epoch 4:   0%|                                                                                                                     | 0/274 [00:00<?, ?it/s][WARNING] PYNATIVE(60206,7ff86dbae0c0,python):2023-03-09-09:57:16.658.729 [mindspore/ccsrc/pipeline/pynative/grad/grad.cc:1198] CheckAlreadyRun] The input info of this cell has changed, forward process will run again\n",
      "Epoch 4: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 274/274 [00:28<00:00,  9.48it/s, loss=0.373]\n",
      "Epoch 4: 100%|████████████████████████████████████████████████████████████████████████████████████| 117/117 [00:08<00:00, 13.37it/s, acc=0.867, loss=0.376]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, imdb_train, epoch)\n",
    "    valid_loss = evaluate(model, imdb_valid, loss_fn, epoch)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        ms.save_checkpoint(model, './sentiment_analysis.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
