{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] ME(49483:139991715923776,MainProcess):2023-05-10-09:02:28.680.215 [mindspore/run_check/_check_version.py:226] Cuda ['10.1', '11.1', '11.6'] version(libcu*.so need by mindspore-gpu) is not found. Please confirm that the path of cuda is set to the env LD_LIBRARY_PATH, or check whether the CUDA version in wheel package and the CUDA runtime in current device matches. Please refer to the installation guidelines: https://www.mindspore.cn/install\n",
      "[ERROR] ME(49483:139991715923776,MainProcess):2023-05-10-09:02:28.703.582 [mindspore/run_check/_check_version.py:226] Cuda ['10.1', '11.1', '11.6'] version(libcudnn*.so need by mindspore-gpu) is not found. Please confirm that the path of cuda is set to the env LD_LIBRARY_PATH, or check whether the CUDA version in wheel package and the CUDA runtime in current device matches. Please refer to the installation guidelines: https://www.mindspore.cn/install\n",
      "/data/miniconda3/envs/mindspore/lib/python3.7/site-packages/mindnlp/utils/download.py:26: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import mindspore\n",
    "import argparse\n",
    "import numpy as np\n",
    "import logging\n",
    "import mindspore.dataset as ds\n",
    "import os\n",
    "\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from mindspore.nn import CrossEntropyLoss\n",
    "from mindspore import nn, ops\n",
    "from mindspore.train.serialization import save_checkpoint\n",
    "from mindnlp.transforms import BertTokenizer\n",
    "from mindnlp.modules import Accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindnlp.utils import cache_file\n",
    "\n",
    "url = 'https://download.mindspore.cn/toolkits/mindnlp/dataset/text_generation/nlpcc2017/train_with_summ.txt'\n",
    "path, _ = cache_file('train_with_summ.txt', './', url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mindspore.dataset import TextFileDataset\n",
    "\n",
    "dataset = TextFileDataset(str(path), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "tokenizer.sep_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def read_map(text):\n",
    "    data = json.loads(text.tobytes())\n",
    "    return np.array(data['article']), np.array(data['summarization'])\n",
    "\n",
    "dataset = dataset.map(read_map, 'text', ['article', 'summary'])\n",
    "dataset = dataset.map(tokenizer, 'article')\n",
    "dataset = dataset.map(tokenizer, 'summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_config = {\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"layer_norm_epsilon\": 1e-05,\n",
    "  \"n_embd\": 768,\n",
    "  \"n_head\": 12,\n",
    "  \"n_ctx\": 1024,\n",
    "  \"n_layer\": 10,\n",
    "  \"num_hidden_layers\": 10,\n",
    "  \"top_k\": 50,\n",
    "  \"top_p\": 1.0,\n",
    "  \"num_return_sequences\": 1,\n",
    "  \"n_positions\": 1024,\n",
    "  \"vocab_size\": 13317\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T07:56:22.837886Z",
     "iopub.status.busy": "2023-04-19T07:56:22.837634Z",
     "iopub.status.idle": "2023-04-19T07:56:22.848643Z",
     "shell.execute_reply": "2023-04-19T07:56:22.848221Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 6\n",
    "batch_size = 8\n",
    "\n",
    "lr = 1e-4\n",
    "warmup_steps = 2000\n",
    "grad_accumulation = 2\n",
    "max_grad_norm = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T07:56:22.867676Z",
     "iopub.status.busy": "2023-04-19T07:56:22.867420Z",
     "iopub.status.idle": "2023-04-19T07:56:22.871045Z",
     "shell.execute_reply": "2023-04-19T07:56:22.870627Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_model(args, vocab_size):\n",
    "    \"\"\"\n",
    "    :param args:\n",
    "    :param vocab_size:字典大小\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if args.pretrained_model:  # 如果指定了预训练的GPT2模型\n",
    "        model = gpt2.GPT2LMHeadModel.load(args.pretrained_model)\n",
    "        # model = gpt2.GPT2LMHeadModel.load(args.pretrained_model)\n",
    "    else:  # 若没有指定预训练模型，则初始化模型\n",
    "        # model_config = config_gpt2.GPT2Config.from_json_file(args.model_config)\n",
    "        model_config = config_gpt2.GPT2Config.from_json(args.model_config)\n",
    "        model = gpt2.GPT2LMHeadModel(config=model_config)\n",
    "    # 根据tokenizer的vocabulary调整GPT2模型的voca的大小\n",
    "    model.resize_token_embeddings(vocab_size)\n",
    "    logger.info('model config:\\n{}'.format(model.config.to_json_string()))\n",
    "    return model, model.config.to_dict().get(\"n_ctx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T07:56:33.262428Z",
     "iopub.status.busy": "2023-04-19T07:56:33.262134Z",
     "iopub.status.idle": "2023-04-19T07:56:33.268067Z",
     "shell.execute_reply": "2023-04-19T07:56:33.267578Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_loss_and_accuracy(outputs, labels):\n",
    "    \"\"\"\n",
    "    计算非pad_id的平均loss和准确率\n",
    "    :param outputs:\n",
    "    :param labels:\n",
    "    :param device:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    logits = outputs[0]  # 每个token用来预测下一个token的prediction_score,维度:[batch_size,token_len,voca_size]\n",
    "    # 用前n-1个token，预测出第n个token\n",
    "    # 用第i个token的prediction_score用来预测第i+1个token。\n",
    "    # 假定有input有n个token，则shift_logits表示model中第[0,n-2]个token的prediction_score，shift_labels表示第[1，n-1]的label\n",
    "    shift_logits = logits[..., :-1, :]\n",
    "    shift_labels = labels[..., 1:]\n",
    "    shift_labels = shift_labels.astype(mindspore.int32)\n",
    "\n",
    "    loss_fct = CrossEntropyLoss(ignore_index=pad_id, reduction='sum')  # 忽略pad_id的loss,并对所有的非pad_id的loss进行求和\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.shape[-1]),\n",
    "                    shift_labels.view(-1))\n",
    "    \n",
    "    preds = shift_logits.argmax(axis=-1) # preds表示对应的prediction_score预测出的token在voca中的id。维度为[batch_size,token_len]\n",
    "\n",
    "    # 对非pad_id的token的loss进行求平均，且计算出预测的准确率\n",
    "    # not_ignore = shift_labels != pad_id\n",
    "    not_ignore = shift_labels.ne(pad_id)  # 进行非运算，返回一个tensor，若targets_view的第i个位置为pad_id，则置为0，否则为1\n",
    "    num_targets = not_ignore.astype(mindspore.int64).sum()  # 计算target中的非pad_id的数量\n",
    "\n",
    "    correct = (shift_labels == preds) & not_ignore  # 计算model预测正确的token的个数，排除pad的tokne\n",
    "    correct = correct.float().sum()\n",
    "\n",
    "    accuracy = correct / num_targets\n",
    "    loss = loss / num_targets\n",
    "    return loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T07:56:33.270280Z",
     "iopub.status.busy": "2023-04-19T07:56:33.269923Z",
     "iopub.status.idle": "2023-04-19T07:56:33.274862Z",
     "shell.execute_reply": "2023-04-19T07:56:33.274397Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define forward function\n",
    "def forward_fn(data, label):\n",
    "    \"\"\"_summary_ 前向推理步骤\n",
    "\n",
    "    Args:\n",
    "        data (_type_): _description_\n",
    "        label (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    logits = model(data)\n",
    "    loss, accuracy = calculate_loss_and_accuracy(logits, label)\n",
    "    return loss / accumulate_step\n",
    "\n",
    "\n",
    "# Get gradient function\n",
    "grad_fn = mindspore.value_and_grad(forward_fn, None, model.trainable_params())\n",
    "\n",
    "\n",
    "# Define function of one-step training\n",
    "# @mindspore.jit\n",
    "def train_step(data, label):\n",
    "    \"\"\"_summary_ 训练步骤\n",
    "\n",
    "    Args:\n",
    "        data (_type_): _description_\n",
    "        label (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    loss, grads = grad_fn(data, label)\n",
    "    loss = ops.depend(loss, accumulator(grads))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下两个函数用于对原始语料进行处理的。\n",
    "第一个preprocess_raw_data函数是根据获取到的文本内容，将其转换为相应的token id。\n",
    "第二个preprocess_mmi_raw_data函数是训练MMI模型，转换的形式与第一个相似。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T08:26:49.528223Z",
     "iopub.status.busy": "2023-04-19T08:26:49.527795Z",
     "iopub.status.idle": "2023-04-19T08:26:49.535798Z",
     "shell.execute_reply": "2023-04-19T08:26:49.535309Z"
    }
   },
   "outputs": [],
   "source": [
    "# 记录模型参数数量\n",
    "num_parameters = 0\n",
    "parameters = model.trainable_params()\n",
    "for parameter in parameters:\n",
    "    num_parameters += parameter.numel()\n",
    "logger.info('number of model parameters: {}'.format(num_parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存模型参数，并对wte进行处理成能识别的训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T08:26:49.538389Z",
     "iopub.status.busy": "2023-04-19T08:26:49.537855Z",
     "iopub.status.idle": "2023-04-19T08:26:49.543223Z",
     "shell.execute_reply": "2023-04-19T08:26:49.542757Z"
    }
   },
   "outputs": [],
   "source": [
    "def ckpt_to_mindspore(mth_file, size:str=None):\n",
    "    \"\"\"_summary_ \n",
    "    Resolve the parameter wte.embedding of the generative model,\n",
    "    the lack of training transformer prefix in table allows for loading\n",
    "\n",
    "    Args:\n",
    "        mth_file (_type_): _description_\n",
    "        size (str, optional): _description_. Defaults to None.\n",
    "\n",
    "    Raises:\n",
    "        ImportError: _description_\n",
    "        RuntimeError: _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import mindspore\n",
    "    except:\n",
    "        raise ImportError(f\"'import mindspore' failed, please install mindspore by \"\n",
    "                          f\"`pip mindspore torch` or instructions from 'https://www.mindspore.cn/install'\")\n",
    "\n",
    "    size = \"mindspore\" if not size else size # rename ckpt\n",
    "\n",
    "    from mindspore import Tensor\n",
    "    from mindspore.train.serialization import save_checkpoint\n",
    "\n",
    "    logging.info('Starting checkpoint conversion.')\n",
    "    ms_ckpt = []\n",
    "    state_dict = mindspore.load_checkpoint(mth_file)\n",
    "\n",
    "    for k, v in state_dict.items():\n",
    "        if 'wte.embedding_table' in k:\n",
    "            k = k.replace('wte.embedding_table', 'transformer.wte.embedding_table')\n",
    "        ms_ckpt.append({'name': k, 'data': Tensor(v.numpy())})\n",
    "\n",
    "    try:\n",
    "        save_checkpoint(ms_ckpt, mth_file)\n",
    "    except:\n",
    "        raise RuntimeError(f'Save checkpoint to {mth_file} failed, please checkout the path.')\n",
    "\n",
    "    return mth_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T08:26:49.545590Z",
     "iopub.status.busy": "2023-04-19T08:26:49.545378Z",
     "iopub.status.idle": "2023-04-19T08:26:49.554940Z",
     "shell.execute_reply": "2023-04-19T08:26:49.554449Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, train_list, multi_gpu, args):\n",
    "    \"\"\"_summary_ 训练逻辑，进行数据加载之后的推理和保存模型\n",
    "\n",
    "    Args:\n",
    "        model (_type_): _description_\n",
    "        train_list (_type_): _description_\n",
    "        multi_gpu (_type_): _description_\n",
    "        args (_type_): _description_\n",
    "\n",
    "    Raises:\n",
    "        exception: _description_\n",
    "    \"\"\"\n",
    "    train_dataset = MyDataset(train_list)\n",
    "    train_dataloader = ds.GeneratorDataset(train_dataset, column_names=\"input_ids\" ,num_parallel_workers=args.num_workers, shuffle=True)\n",
    "    train_dataloader = train_dataloader.padded_batch(batch_size=args.batch_size, drop_remainder=True, pad_info={})\n",
    "    train_dataloader = train_dataloader.repeat(1)\n",
    "    train_dataloader = train_dataloader.shuffle(10)\n",
    "    \n",
    "    # 计算所有epoch进行参数优化的总步数total_steps\n",
    "    total_steps = int(train_dataset.__len__() * args.epochs / args.batch_size / args.gradient_accumulation)\n",
    "    logger.info('total training steps = {}'.format(total_steps))\n",
    "\n",
    "    logger.info('starting training')\n",
    "    # 记录 out of memory的次数\n",
    "    oom_time = 0\n",
    "    # 开始训练\n",
    "    for epoch in range(args.epochs):\n",
    "        epoch_start_time = datetime.now()\n",
    "        size = len(train_dataloader)\n",
    "        # batch_idx为int，input_ids为一个tensor\n",
    "        for batch_idx, input_ids in enumerate(train_dataloader.create_tuple_iterator()):\n",
    "            # 注意：GPT2模型的construct()函数，是对于给定的context，生成一个token，而不是生成一串token\n",
    "            # GPT2Model的输入为n个token_id时，输出也是n个hidden_state，使用第n个hidden_state预测第n+1个token\n",
    "            # 解决在运行过程中，由于显存不足产生的cuda out of memory的问题\n",
    "            try:\n",
    "                input_id = input_ids[0].astype(mindspore.int64)\n",
    "                loss = train_step(input_id, input_id)\n",
    "                \n",
    "                if batch_idx % args.log_step == 0:\n",
    "                    loss, current = loss.asnumpy(), batch_idx\n",
    "                    logger.info(f\"loss: {loss:>7f}  [{current:>3d}/{size:>3d}]\")\n",
    "            except RuntimeError as exception:\n",
    "                if \"out of memory\" in str(exception):\n",
    "                    oom_time += 1\n",
    "                    logger.info(\"WARNING: ran out of memory,times: {}\".format(oom_time))    \n",
    "                else:\n",
    "                    logger.info(str(exception))\n",
    "                    raise exception\n",
    "        logger.info('saving model for epoch {}'.format(epoch + 1))\n",
    "        if args.train_mmi:  # 当前训练MMI模型\n",
    "            model_path = join(args.mmi_model_output_path, 'model_epoch{}'.format(epoch + 1))\n",
    "        else:  # 当前训练对话模型\n",
    "            model_path = join(args.dialogue_model_output_path, 'model_epoch{}'.format(epoch + 1))\n",
    "        if not os.path.exists(model_path):\n",
    "            os.mkdir(model_path)\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model\n",
    "        model_path = f\"{model_path}/mindspore_model.ckpt\"\n",
    "        save_checkpoint(model_to_save, model_path)\n",
    "        ckpt_to_mindspore(model_path)\n",
    "        logger.info('epoch {} finished'.format(epoch + 1))\n",
    "        epoch_finish_time = datetime.now()\n",
    "        logger.info('time for one epoch: {}'.format(epoch_finish_time - epoch_start_time))\n",
    "    logger.info('training finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T08:26:49.557325Z",
     "iopub.status.busy": "2023-04-19T08:26:49.556863Z",
     "iopub.status.idle": "2023-04-19T08:26:50.029863Z",
     "shell.execute_reply": "2023-04-19T08:26:50.029250Z"
    }
   },
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "logger.info(\"loading traing data\")\n",
    "if args.train_mmi:  # 如果是训练MMI模型\n",
    "    with open(args.train_mmi_tokenized_path, \"r\", encoding=\"utf8\") as f:\n",
    "        data = f.read()\n",
    "else:  # 如果是训练对话生成模型\n",
    "    with open(args.train_tokenized_path, \"r\", encoding=\"utf8\") as f:\n",
    "        data = f.read()\n",
    "data_list = [line.rstrip() for line in data.split(\"\\n\") if line.rstrip()]\n",
    "train_list = data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-19T08:26:50.032628Z",
     "iopub.status.busy": "2023-04-19T08:26:50.032311Z",
     "iopub.status.idle": "2023-04-19T22:22:05.182820Z",
     "shell.execute_reply": "2023-04-19T22:22:05.182184Z"
    }
   },
   "outputs": [],
   "source": [
    "# 开始训练\n",
    "train(model, train_list, multi_gpu, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
