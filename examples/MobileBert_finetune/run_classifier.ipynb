{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileBert 下游任务\n",
    "\n",
    "## 数据准备\n",
    "我们本次使用的数据集为**GLUE数据集**，GLUE包含九项NLU任务，语言均为英语。GLUE九项任务涉及到自然语言推断、文本蕴含、情感分析、语义相似等多个任务，本下游任务主要使用的是其中的SST-2。\n",
    "\n",
    "SST-2(The Stanford Sentiment Treebank，斯坦福情感树库)，单句子分类任务，包含电影评论中的句子和它们情感的人类注释。这项任务是给定句子的情感，类别分为两类正面情感（positive，样本标签对应为1）和负面情感（negative，样本标签对应为0），并且只用句子级别的标签。也就是，本任务也是一个二分类任务，针对句子级别，分为正面和负面情感。\n",
    "\n",
    "- 样本个数：训练集67, 350个，开发集873个，测试集1, 821个。\n",
    "- 任务：情感分类，正面情感和负面情感二分类。\n",
    "- 评价准则：accuracy。\n",
    "### 数据下载\n",
    "运行tools/download_glue_data.py并指定输出文件夹即可下载数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Script for downloading all GLUE data.\n",
    "Note: for legal reasons, we are unable to host MRPC.\n",
    "You can either use the version hosted by the SentEval team, which is already tokenized,\n",
    "or you can download the original data from (https://download.microsoft.com/download/D/4/6/D46FF87A-F6B9-4252-AA8B-3604ED519838/MSRParaphraseCorpus.msi) and extract the data from it manually.\n",
    "For Windows users, you can run the .msi file. For Mac and Linux users, consider an external library such as 'cabextract' (see below for an example).\n",
    "You should then rename and place specific files in a folder (see below for an example).\n",
    "mkdir MRPC\n",
    "cabextract MSRParaphraseCorpus.msi -d MRPC\n",
    "cat MRPC/_2DEC3DBE877E4DB192D17C0256E90F1D | tr -d $'\\r' > MRPC/msr_paraphrase_train.txt\n",
    "cat MRPC/_D7B391F9EAFF4B1B8BCE8F21B20B1B61 | tr -d $'\\r' > MRPC/msr_paraphrase_test.txt\n",
    "rm MRPC/_*\n",
    "rm MSRParaphraseCorpus.msi\n",
    "'''\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import argparse\n",
    "import tempfile\n",
    "import urllib\n",
    "import io\n",
    "if sys.version_info >= (3, 0):\n",
    "    import urllib.request\n",
    "import zipfile\n",
    "\n",
    "URLLIB=urllib\n",
    "if sys.version_info >= (3, 0):\n",
    "    URLLIB=urllib.request\n",
    "\n",
    "TASKS = [\"CoLA\", \"SST\", \"MRPC\", \"QQP\", \"STS\", \"MNLI\", \"SNLI\", \"QNLI\", \"RTE\", \"WNLI\", \"diagnostic\"]\n",
    "TASK2PATH = {\"CoLA\":'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FCoLA.zip?alt=media&token=46d5e637-3411-4188-bc44-5809b5bfb5f4',\n",
    "             \"SST\":'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSST-2.zip?alt=media&token=aabc5f6b-e466-44a2-b9b4-cf6337f84ac8',\n",
    "             \"MRPC\":'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2Fmrpc_dev_ids.tsv?alt=media&token=ec5c0836-31d5-48f4-b431-7480817f1adc',\n",
    "             \"QQP\":'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FQQP.zip?alt=media&token=700c6acf-160d-4d89-81d1-de4191d02cb5',\n",
    "             \"STS\":'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSTS-B.zip?alt=media&token=bddb94a7-8706-4e0d-a694-1109e12273b5',\n",
    "             \"MNLI\":'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FMNLI.zip?alt=media&token=50329ea1-e339-40e2-809c-10c40afff3ce',\n",
    "             \"SNLI\":'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSNLI.zip?alt=media&token=4afcfbb2-ff0c-4b2d-a09a-dbf07926f4df',\n",
    "             \"QNLI\":'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FQNLI.zip?alt=media&token=c24cad61-f2df-4f04-9ab6-aa576fa829d0',\n",
    "             \"RTE\":'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FRTE.zip?alt=media&token=5efa7e85-a0bb-4f19-8ea2-9e1840f077fb',\n",
    "             \"WNLI\":'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FWNLI.zip?alt=media&token=068ad0a0-ded7-4bd7-99a5-5e00222e0faf',\n",
    "             \"diagnostic\":'https://storage.googleapis.com/mtl-sentence-representations.appspot.com/tsvsWithoutLabels%2FAX.tsv?GoogleAccessId=firebase-adminsdk-0khhl@mtl-sentence-representations.iam.gserviceaccount.com&Expires=2498860800&Signature=DuQ2CSPt2Yfre0C%2BiISrVYrIFaZH1Lc7hBVZDD4ZyR7fZYOMNOUGpi8QxBmTNOrNPjR3z1cggo7WXFfrgECP6FBJSsURv8Ybrue8Ypt%2FTPxbuJ0Xc2FhDi%2BarnecCBFO77RSbfuz%2Bs95hRrYhTnByqu3U%2FYZPaj3tZt5QdfpH2IUROY8LiBXoXS46LE%2FgOQc%2FKN%2BA9SoscRDYsnxHfG0IjXGwHN%2Bf88q6hOmAxeNPx6moDulUF6XMUAaXCSFU%2BnRO2RDL9CapWxj%2BDl7syNyHhB7987hZ80B%2FwFkQ3MEs8auvt5XW1%2Bd4aCU7ytgM69r8JDCwibfhZxpaa4gd50QXQ%3D%3D'}\n",
    "\n",
    "MRPC_TRAIN = 'https://s3.amazonaws.com/senteval/senteval_data/msr_paraphrase_train.txt'\n",
    "MRPC_TEST = 'https://s3.amazonaws.com/senteval/senteval_data/msr_paraphrase_test.txt'\n",
    "\n",
    "def download_and_extract(task, data_dir):\n",
    "    print(\"Downloading and extracting %s...\" % task)\n",
    "    data_file = \"%s.zip\" % task\n",
    "    URLLIB.urlretrieve(TASK2PATH[task], data_file)\n",
    "    with zipfile.ZipFile(data_file) as zip_ref:\n",
    "        zip_ref.extractall(data_dir)\n",
    "    os.remove(data_file)\n",
    "    print(\"\\tCompleted!\")\n",
    "\n",
    "def format_mrpc(data_dir, path_to_data):\n",
    "    print(\"Processing MRPC...\")\n",
    "    mrpc_dir = os.path.join(data_dir, \"MRPC\")\n",
    "    if not os.path.isdir(mrpc_dir):\n",
    "        os.mkdir(mrpc_dir)\n",
    "    if path_to_data:\n",
    "        mrpc_train_file = os.path.join(path_to_data, \"msr_paraphrase_train.txt\")\n",
    "        mrpc_test_file = os.path.join(path_to_data, \"msr_paraphrase_test.txt\")\n",
    "    else:\n",
    "        mrpc_train_file = os.path.join(mrpc_dir, \"msr_paraphrase_train.txt\")\n",
    "        mrpc_test_file = os.path.join(mrpc_dir, \"msr_paraphrase_test.txt\")\n",
    "        URLLIB.urlretrieve(MRPC_TRAIN, mrpc_train_file)\n",
    "        URLLIB.urlretrieve(MRPC_TEST, mrpc_test_file)\n",
    "    assert os.path.isfile(mrpc_train_file), \"Train data not found at %s\" % mrpc_train_file\n",
    "    assert os.path.isfile(mrpc_test_file), \"Test data not found at %s\" % mrpc_test_file\n",
    "    URLLIB.urlretrieve(TASK2PATH[\"MRPC\"], os.path.join(mrpc_dir, \"dev_ids.tsv\"))\n",
    "\n",
    "    dev_ids = []\n",
    "    with io.open(os.path.join(mrpc_dir, \"dev_ids.tsv\"), encoding='utf-8') as ids_fh:\n",
    "        for row in ids_fh:\n",
    "            dev_ids.append(row.strip().split('\\t'))\n",
    "\n",
    "    with io.open(mrpc_train_file, encoding='utf-8') as data_fh, \\\n",
    "         io.open(os.path.join(mrpc_dir, \"train.tsv\"), 'w', encoding='utf-8') as train_fh, \\\n",
    "         io.open(os.path.join(mrpc_dir, \"dev.tsv\"), 'w', encoding='utf-8') as dev_fh:\n",
    "        header = data_fh.readline()\n",
    "        train_fh.write(header)\n",
    "        dev_fh.write(header)\n",
    "        for row in data_fh:\n",
    "            label, id1, id2, s1, s2 = row.strip().split('\\t')\n",
    "            if [id1, id2] in dev_ids:\n",
    "                dev_fh.write(\"%s\\t%s\\t%s\\t%s\\t%s\\n\" % (label, id1, id2, s1, s2))\n",
    "            else:\n",
    "                train_fh.write(\"%s\\t%s\\t%s\\t%s\\t%s\\n\" % (label, id1, id2, s1, s2))\n",
    "\n",
    "    with io.open(mrpc_test_file, encoding='utf-8') as data_fh, \\\n",
    "            io.open(os.path.join(mrpc_dir, \"test.tsv\"), 'w', encoding='utf-8') as test_fh:\n",
    "        header = data_fh.readline()\n",
    "        test_fh.write(\"index\\t#1 ID\\t#2 ID\\t#1 String\\t#2 String\\n\")\n",
    "        for idx, row in enumerate(data_fh):\n",
    "            label, id1, id2, s1, s2 = row.strip().split('\\t')\n",
    "            test_fh.write(\"%d\\t%s\\t%s\\t%s\\t%s\\n\" % (idx, id1, id2, s1, s2))\n",
    "    print(\"\\tCompleted!\")\n",
    "\n",
    "def download_diagnostic(data_dir):\n",
    "    print(\"Downloading and extracting diagnostic...\")\n",
    "    if not os.path.isdir(os.path.join(data_dir, \"diagnostic\")):\n",
    "        os.mkdir(os.path.join(data_dir, \"diagnostic\"))\n",
    "    data_file = os.path.join(data_dir, \"diagnostic\", \"diagnostic.tsv\")\n",
    "    URLLIB.urlretrieve(TASK2PATH[\"diagnostic\"], data_file)\n",
    "    print(\"\\tCompleted!\")\n",
    "    return\n",
    "\n",
    "def get_tasks(task_names):\n",
    "    task_names = task_names.split(',')\n",
    "    if \"all\" in task_names:\n",
    "        tasks = TASKS\n",
    "    else:\n",
    "        tasks = []\n",
    "        for task_name in task_names:\n",
    "            assert task_name in TASKS, \"Task %s not found!\" % task_name\n",
    "            tasks.append(task_name)\n",
    "    return tasks\n",
    "\n",
    "def main(arguments):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-d', '--data_dir', help='directory to save data to', type=str, default='glue_data')\n",
    "    parser.add_argument('-t', '--tasks', help='tasks to download data for as a comma separated string',\n",
    "                        type=str, default='all')\n",
    "    parser.add_argument('--path_to_mrpc', help='path to directory containing extracted MRPC data, msr_paraphrase_train.txt and msr_paraphrase_text.txt',\n",
    "                        type=str, default='')\n",
    "    args = parser.parse_args(arguments)\n",
    "\n",
    "    if not os.path.isdir(args.data_dir):\n",
    "        os.mkdir(args.data_dir)\n",
    "    tasks = get_tasks(args.tasks)\n",
    "\n",
    "    for task in tasks:\n",
    "        if task == 'MRPC':\n",
    "            format_mrpc(args.data_dir, args.path_to_mrpc)\n",
    "        elif task == 'diagnostic':\n",
    "            download_diagnostic(args.data_dir)\n",
    "        else:\n",
    "            download_and_extract(task, args.data_dir)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sys.exit(main(sys.argv[1:]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预训练模型\n",
    "将`config.json` 和 `vocab.txt` 以及预训练权重都放入prev_trained_model文件夹"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练与推理代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Finetuning the library models for sequence classification .\"\"\"\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import mindspore\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.ops as ops\n",
    "from mindspore.dataset import text\n",
    "from mindspore import nn, Tensor, load_checkpoint, save_checkpoint\n",
    "from mindspore.dataset import RandomSampler,  DistributedSampler, NumpySlicesDataset, SequentialSampler\n",
    "from mindspore.communication.management import init, get_group_size\n",
    "\n",
    "from mindnlp.models.mobilebert.mobilebert import MobileBertForSequenceClassification\n",
    "from mindnlp.models.mobilebert.mobilebert_config import MobileBertConfig\n",
    "\n",
    "from mindnlp.transforms.tokenizers.bert_tokenizer import BertTokenizer\n",
    "\n",
    "from metrics.glue_compute_metrics import compute_metrics\n",
    "from processors import glue_output_modes as output_modes\n",
    "from processors import glue_processors as processors\n",
    "from processors import glue_convert_examples_to_features as convert_examples_to_features\n",
    "from processors import collate_fn\n",
    "from tools.common import init_logger, logger\n",
    "from callback.progressbar import ProgressBar\n",
    "from tools.finetuning_argparse import get_argparse\n",
    "\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    \"mobilebert\": (MobileBertConfig, MobileBertForSequenceClassification, BertTokenizer),\n",
    "}\n",
    "\n",
    "def train(args, train_dataset, model):\n",
    "    \"\"\" Train the model \"\"\"\n",
    "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "\n",
    "    train_sampler = RandomSampler()\n",
    "    train_dataloader =NumpySlicesDataset(train_dataset, sampler=train_sampler)\n",
    "    train_dataloader = train_dataloader.batch(args.train_batch_size)\n",
    "    if args.max_steps > 0:\n",
    "        num_training_steps = args.max_steps\n",
    "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
    "    else:\n",
    "        num_training_steps = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "    args.warmup_steps = int(num_training_steps * args.warmup_proportion)\n",
    "    # Prepare optimizer (linear warmup and decay)\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': list(filter(lambda x: 'bias' not in x.name and 'LayerNorm.weight' not in x.name, model.trainable_params())),\n",
    "         'weight_decay': args.weight_decay},\n",
    "        {'params': list(filter(lambda x: 'bias' in x.name or 'LayerNorm.weight' in x.name, model.trainable_params())), 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = nn.AdamWeightDecay(optimizer_grouped_parameters, learning_rate=args.learning_rate, eps=args.adam_epsilon)\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
    "\n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset[0]))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
    "    logger.info(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "                args.train_batch_size * args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", num_training_steps)\n",
    "\n",
    "    global_step = 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    for _ in range(int(args.num_train_epochs)):\n",
    "        pbar = ProgressBar(n_total=len(train_dataloader), desc='Training')\n",
    "        def forward_fn(data, label):\n",
    "            logits = model(**data)[1]\n",
    "            loss = loss_fn(logits.view(-1, 2), label.view(-1))\n",
    "            return loss, logits\n",
    "\n",
    "        # Get gradient function\n",
    "        grad_fn = mindspore.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)\n",
    "\n",
    "        # Define function of one-step training\n",
    "        def train_step(data, label):\n",
    "            (loss, _), grads = grad_fn(data, label)\n",
    "            loss = ops.depend(loss, optimizer(ops.clip_by_global_norm(grads, args.max_grad_norm)))\n",
    "            return loss\n",
    "        model.set_train()\n",
    "        for batch, (all_input_ids, all_attention_mask, all_token_type_ids, all_lens, all_labels ) in enumerate(train_dataloader):\n",
    "            inputs = {'input_ids': all_input_ids,\n",
    "                      'attention_mask': all_attention_mask,\n",
    "                      'labels': Tensor(all_labels, mindspore.int32)}\n",
    "            inputs['token_type_ids'] = all_token_type_ids\n",
    "            loss = train_step(inputs, Tensor(all_labels, mindspore.int32))\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "            tr_loss += loss\n",
    "            if (batch + 1) % args.gradient_accumulation_steps == 0:\n",
    "                global_step += 1\n",
    "            if args.save_steps > 0 and global_step % args.save_steps == 0:\n",
    "                # Save model checkpoint\n",
    "                output_dir = os.path.join(args.output_dir, 'checkpoint-{}'.format(global_step//10))\n",
    "                if not os.path.exists(output_dir):\n",
    "                    os.makedirs(output_dir)\n",
    "                save_checkpoint(model, os.path.join(output_dir, 'mobilebert-uncased.ckpt'))\n",
    "                logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "            pbar(batch, {'loss': loss.asnumpy()})\n",
    "        print(\" \")\n",
    "    return global_step, tr_loss / global_step, inputs\n",
    "\n",
    "\n",
    "def evaluate(args, model, tokenizer, prefix=\"\"):\n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "    eval_task_names = (\"mnli\", \"mnli-mm\") if args.task_name == \"mnli\" else (args.task_name,)\n",
    "    eval_outputs_dirs = (args.output_dir, args.output_dir + '-MM') if args.task_name == \"mnli\" else (args.output_dir,)\n",
    "\n",
    "    results = {}\n",
    "    for eval_task, eval_output_dir in zip(eval_task_names, eval_outputs_dirs):\n",
    "        eval_dataset = load_and_cache_examples(args, eval_task, tokenizer, data_type='dev')\n",
    "        if not os.path.exists(eval_output_dir):\n",
    "            os.makedirs(eval_output_dir)\n",
    "\n",
    "        args.eval_batch_size = args.per_gpu_eval_batch_size * args.n_gpu\n",
    "        # Note that DistributedSampler samples randomly\n",
    "        eval_sampler = SequentialSampler()\n",
    "        eval_dataloader = NumpySlicesDataset(eval_dataset, sampler=eval_sampler)\n",
    "        eval_dataloader = eval_dataloader.batch(args.eval_batch_size)\n",
    "\n",
    "        # Eval!\n",
    "        logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "        logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "        logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "        eval_loss = 0.0\n",
    "        nb_eval_steps = 0\n",
    "        preds = None\n",
    "        out_label_ids = None\n",
    "        pbar = ProgressBar(n_total=len(eval_dataloader), desc=\"Evaluating\")\n",
    "        model.set_train(False)\n",
    "        model.set_grad(False)\n",
    "        for batch, (all_input_ids, all_attention_mask, all_token_type_ids, all_lens, all_labels ) in enumerate(eval_dataloader):\n",
    "            inputs = {'input_ids': all_input_ids,\n",
    "                      'attention_mask': all_attention_mask,\n",
    "                      'labels': Tensor(all_labels, mindspore.int32)}\n",
    "            inputs['token_type_ids'] = all_token_type_ids\n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "            eval_loss += tmp_eval_loss\n",
    "            nb_eval_steps += 1\n",
    "            if preds is None:\n",
    "                preds = logits\n",
    "                out_label_ids = inputs['labels']\n",
    "            else:\n",
    "                preds = np.append(preds, logits.asnumpy(), axis=0)\n",
    "                out_label_ids = np.append(out_label_ids, inputs['labels'].asnumpy(), axis=0)\n",
    "            pbar(batch)\n",
    "        print(' ')\n",
    "        eval_loss = eval_loss / nb_eval_steps\n",
    "        if args.output_mode == \"classification\":\n",
    "            preds = np.argmax(preds, axis=1)\n",
    "        elif args.output_mode == \"regression\":\n",
    "            preds = np.squeeze(preds)\n",
    "        result = compute_metrics(eval_task, preds, out_label_ids)\n",
    "        results.update(result)\n",
    "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "    return results\n",
    "\n",
    "\n",
    "def load_and_cache_examples(args, task, tokenizer, data_type='train'):\n",
    "    \"\"\"load_and_cache_examples\"\"\"\n",
    "    processor = processors[task]()\n",
    "    output_mode = output_modes[task]\n",
    "    # Load data features from dataset file\n",
    "    logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n",
    "    label_list = processor.get_labels()\n",
    "    if task in ['mnli', 'mnli-mm'] and 'roberta' in args.model_type:\n",
    "        # HACK(label indices are swapped in RoBERTa pretrained model)\n",
    "        label_list[1], label_list[2] = label_list[2], label_list[1]\n",
    "\n",
    "    if data_type == 'train':\n",
    "        examples = processor.get_train_examples(args.data_dir)\n",
    "    elif data_type == 'dev':\n",
    "        examples = processor.get_dev_examples(args.data_dir)\n",
    "    else:\n",
    "        examples = processor.get_test_examples(args.data_dir)\n",
    "\n",
    "    features = convert_examples_to_features(examples,\n",
    "                                            tokenizer,\n",
    "                                            label_list=label_list,\n",
    "                                            max_seq_length=args.max_seq_length,\n",
    "                                            output_mode=output_mode)\n",
    "\n",
    "    # Convert to Tensors and build dataset\n",
    "    all_input_ids = [f.input_ids for f in features]\n",
    "    all_attention_mask = [f.attention_mask for f in features]\n",
    "    all_token_type_ids = [f.token_type_ids for f in features]\n",
    "    all_lens = [f.input_len for f in features]\n",
    "    if output_mode == \"classification\":\n",
    "        all_labels = [f.label for f in features]\n",
    "    elif output_mode == \"regression\":\n",
    "        all_labels = [f.label for f in features]\n",
    "    dataset = ((all_input_ids, all_attention_mask, all_token_type_ids, all_lens, all_labels))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = get_argparse()\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.mkdir(args.output_dir)\n",
    "    args.output_dir = args.output_dir + '{}'.format(args.model_type)\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.mkdir(args.output_dir)\n",
    "    time_ = time.strftime(\"%Y-%m-%d-%H:%M:%S\", time.localtime())\n",
    "    init_logger(log_file=args.output_dir + f'/{args.model_type}-{args.task_name}-{time_}.log')\n",
    "    if os.path.exists(args.output_dir) and os.listdir(\n",
    "            args.output_dir) and args.do_train and not args.overwrite_output_dir:\n",
    "        raise ValueError(\n",
    "            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
    "                args.output_dir))\n",
    "\n",
    "    # Setup distant debugging if needed\n",
    "    if args.server_ip and args.server_port:\n",
    "        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
    "        import ptvsd\n",
    "        print(\"Waiting for debugger attach\")\n",
    "        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
    "        ptvsd.wait_for_attach()\n",
    "\n",
    "    args.n_gpu = 1\n",
    "    # Prepare GLUE task\n",
    "    args.task_name = args.task_name.lower()\n",
    "    if args.task_name not in processors:\n",
    "        raise ValueError(\"Task not found: %s\" % (args.task_name))\n",
    "    processor = processors[args.task_name]()\n",
    "    args.output_mode = output_modes[args.task_name]\n",
    "    label_list = processor.get_labels()\n",
    "    num_labels = len(label_list)\n",
    "\n",
    "    # Load pretrained model and tokenizer\n",
    "    args.model_type = args.model_type.lower()\n",
    "    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "    config = config_class.from_pretrained(\n",
    "        args.config_name if args.config_name else args.model_name_or_path,\n",
    "        num_labels=num_labels,\n",
    "        finetuning_task=args.task_name,\n",
    "        cache_dir=args.cache_dir if args.cache_dir else None,\n",
    "    )\n",
    "    print(args)\n",
    "    vocab = text.Vocab.from_file(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path+\"/vocab.txt\")\n",
    "    tokenizer = tokenizer_class(\n",
    "        vocab,\n",
    "        lower_case=args.do_lower_case,\n",
    "    )\n",
    "    model = model_class.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
    "        config=config,\n",
    "        cache_dir=args.cache_dir if args.cache_dir else None,\n",
    "    )\n",
    "    logger.info(\"Training/evaluation parameters %s\", args)\n",
    "\n",
    "    # Training\n",
    "    if args.do_train:\n",
    "        train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, data_type='train')\n",
    "        global_step, tr_loss, inputs = train(args, train_dataset, model)\n",
    "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "\n",
    "    # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "    if args.do_train:\n",
    "        # Create output directory if needed\n",
    "        if not os.path.exists(args.output_dir):\n",
    "            os.makedirs(args.output_dir)\n",
    "\n",
    "        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
    "        save_checkpoint(model, os.path.join(args.output_dir, 'mobilebert-uncased.ckpt'))\n",
    "\n",
    "    if args.do_eval:\n",
    "        print(evaluate(args, model, tokenizer))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练\n",
    "运行脚本 scripts/run_classifier_sst2.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_DIR=`pwd`\n",
    "export BERT_BASE_DIR=$CURRENT_DIR/prev_trained_model/mobilebert\n",
    "export DATA_DIR=$CURRENT_DIR/dataset/glue_data\n",
    "export OUTPUR_DIR=$CURRENT_DIR/outputs\n",
    "export CUDA_VISIBLE_DEVICES=5\n",
    "TASK_NAME=\"SST-2\"\n",
    "\n",
    "python run_classifier.py \\\n",
    "  --model_type=mobilebert \\\n",
    "  --model_name_or_path=$BERT_BASE_DIR \\\n",
    "  --task_name=$TASK_NAME \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --do_lower_case \\\n",
    "  --data_dir=$DATA_DIR/${TASK_NAME}/ \\\n",
    "  --max_seq_length=128 \\\n",
    "  --per_gpu_train_batch_size=32 \\\n",
    "  --per_gpu_eval_batch_size=32 \\\n",
    "  --learning_rate=5e-5 \\\n",
    "  --num_train_epochs=5.0 \\\n",
    "  --max_grad_norm=1.0 \\\n",
    "  --logging_steps=2105 \\\n",
    "  --save_steps=2105 \\\n",
    "  --output_dir=$OUTPUR_DIR/${TASK_NAME}_output/ \\\n",
    "  --overwrite_output_dir \\\n",
    "  --seed=42\\\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
